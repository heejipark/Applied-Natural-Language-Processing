{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hee Ji Park (4090715830) - HW3\n",
    "- Python version 3.6.12\n",
    "- Jupyter notebook version 6.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task1 : Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the word is number, return True. Or return False\n",
    "def isNumber(s):\n",
    "    try:\n",
    "        if ',' in s: # ex) 4,800 -> 4800\n",
    "            s = s.replace(',','')\n",
    "        float(s) \n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punct = set(string.punctuation) \n",
    "noun_suffix = [\"let\",'ie',\"kin\",\"action\", \"ling\", \"hood\", \"ship\", \"ary\",\"age\",\n",
    "               \"ery\", \"ory\", \"ance\", \"an\",\"ary\",\"eer\",\"er\",\"ier\",\"herd\",\"cy\", \"dom\", \n",
    "               \"ee\", \"ence\", \"ster\", \"yer\", \"ant\",\"ar\", \"ion\", \"ism\", \"ist\", \"ity\", \n",
    "               \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ize\", \"ise\"]\n",
    "adj_suffix = [\"able\", \"ible\", 'ant', 'ent', 'ive', \"al\",\"ial\",\"an\",\"ian\",\"ish\",\n",
    "              \"ern\", \"ese\", \"ful\", 'ar', 'ary','ly','less','ic','ive','ous', \"i\", \"ic\"]\n",
    "adv_suffix = [\"ly\",\"lng\",\"ward\", \"wards\", \"way\", \"ways\", \"wise\"]\n",
    "\n",
    "def unk_preprocessing(s):\n",
    "    # If unknown word has number, return <unk_num> token\n",
    "    if any(char.isdigit() for char in s):\n",
    "        return \"<unk_num>\"\n",
    "    \n",
    "    # If unknown word has punctuation, return <unk_punt> token\n",
    "    elif any(char in punct for char in s):\n",
    "        return \"<unk_punct>\"\n",
    "\n",
    "    # If unknown word has upper characters, return <unk_upper> token\n",
    "    elif any(char.isupper() for char in s):\n",
    "        return \"<unk_upper>\"\n",
    "\n",
    "    # If unknown word contains characteristics of noun, return <unk_noun> token\n",
    "    elif any(s.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"<unk_noun>\"\n",
    "\n",
    "    # If unknown word contains characteristics of verb, return <unk_verb> token\n",
    "    elif any(s.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"<unk_verb>\"\n",
    "\n",
    "    # If unknown word contains characteristics of adj, return <unk_adj> token\n",
    "    elif any(s.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"<unk_adj>\"\n",
    "\n",
    "    # If unknown word contains characteristics of adverbs, return <unk_adv> token\n",
    "    elif any(s.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"<unk_adv>\"\n",
    "    \n",
    "    else:\n",
    "        return \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vovabulary Creation\n",
    "def vocab_creation(file, min_count):\n",
    "    vocab = {}\n",
    "    tag = {}\n",
    "    with open(file, \"r\") as train:\n",
    "        for line in train:\n",
    "            if not line.split(): # Ignore a blank line\n",
    "                continue\n",
    "            word, wordtag = line.split(\"\\t\")[1], line.split(\"\\t\")[2].strip('\\n')\n",
    "            if isNumber(word): # if word is number, change number to '<num>' token\n",
    "                word = '<num>'\n",
    "            if word not in vocab: # store {vocabrary : frequency}\n",
    "                vocab[word] = 1\n",
    "            else:\n",
    "                vocab[word] += 1\n",
    "            if wordtag not in tag: # store {tag : frequency}\n",
    "                tag[wordtag] = 1\n",
    "            else:\n",
    "                tag[wordtag] += 1\n",
    "                \n",
    "\n",
    "        # make <unk> token\n",
    "        vocab['<unk>'], vocab['<unk_num>'] = 0,0\n",
    "        vocab['<unk_punct>'], vocab[\"<unk_upper>\"] = 0,0\n",
    "        vocab[\"<unk_noun>\"], vocab[\"<unk_verb>\"] = 0,0\n",
    "        vocab[\"<unk_adj>\"], vocab[\"<unk_adv>\"] = 0,0\n",
    "        \n",
    "        delete = []\n",
    "        for word, occurrences in vocab.items():\n",
    "            if occurrences >= min_count: # If occurrences is bigger than 3 : Pass\n",
    "                continue\n",
    "            else:\n",
    "                new_tag = unk_preprocessing(word)\n",
    "                vocab[new_tag] += occurrences   # If occurrences is lower than 3 : change word name to < unk >\n",
    "                delete.append(word) # To remove the word in the dictionary (vocab), store 'word' in the delete list\n",
    "\n",
    "        for i in delete:  \n",
    "            del vocab[i] # Remove the word in the vocab dictionary\n",
    "        # For save unk token and corresponding occurrences of <unk> token\n",
    "        unk1 = ('<unk>',vocab['<unk>']) \n",
    "        unk2 = ('<unk_num>',vocab['<unk_num>'])\n",
    "        unk3 = ('<unk_punct>',vocab['<unk_punct>'])\n",
    "        unk4 = ('<unk_upper>',vocab['<unk_upper>'])\n",
    "        unk5 = ('<unk_noun>',vocab['<unk_noun>']) \n",
    "        unk6 = ('<unk_verb>',vocab['<unk_verb>'])\n",
    "        unk7 = ('<unk_adj>',vocab['<unk_adj>'])\n",
    "        unk8 = ('<unk_adv>',vocab['<unk_adv>'])\n",
    "        \n",
    "        # Remove <unk> token in the vocab, because we have to put <unk> token on the top rows\n",
    "        del vocab['<unk>'] \n",
    "        del vocab['<unk_num>']\n",
    "        del vocab['<unk_punct>']\n",
    "        del vocab['<unk_upper>']\n",
    "        del vocab['<unk_noun>']\n",
    "        del vocab['<unk_verb>']\n",
    "        del vocab['<unk_adj>']\n",
    "        del vocab['<unk_adv>']\n",
    "        \n",
    "        # Sort the occurrences in the dict\n",
    "        vocab_to_list = sorted(vocab.items(), key=lambda x: x[1], reverse=True) \n",
    "        \n",
    "        # Put <unk> token on the top rows\n",
    "        \n",
    "        tot_unk = [unk8,unk7,unk6,unk5,unk4,unk3,unk2,unk1]\n",
    "        tot_unk = sorted(tot_unk, key=lambda x: x[1], reverse=True)\n",
    "        for unk in tot_unk[::-1]:\n",
    "            vocab_to_list.insert(0,unk)\n",
    "\n",
    "        # Make a vocab text file\n",
    "        with open('vocab.txt', \"w\") as out:\n",
    "            for idx,line in enumerate(vocab_to_list):\n",
    "                out.write(\"{0}\\t{1}\\t{2}\\n\".format(line[0],idx,line[1]))\n",
    "            out.close()\n",
    "\n",
    "        return vocab_to_list, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, tag = vocab_creation('./data/train',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the selected threshold for unknown words replacement? : 2\n",
      "What is the total size of your vocabulary? : 21578\n",
      "What is the total occurrences of the special token '<unk>' after replacement? : 17343\n"
     ]
    }
   ],
   "source": [
    "print(\"What is the selected threshold for unknown words replacement? : 2\")\n",
    "print(\"What is the total size of your vocabulary? : %d\" % len(vocab))\n",
    "print(\"What is the total occurrences of the special token '<unk>' after replacement? : %d\" % (vocab[0][1] + vocab[1][1] + vocab[2][1] + vocab[3][1] + vocab[4][1] + vocab[5][1] + vocab[6][1] + vocab[7][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task 2 : Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tag sentences for transition\n",
    "def make_sentences_for_transition(file):\n",
    "    for_transition = []\n",
    "    with open(file, \"r\") as train:\n",
    "        sentence = 'Φ'\n",
    "        for line in train:\n",
    "            if not line.split(): # In case of a blank line, make new sentence\n",
    "                for_transition.append(sentence)\n",
    "                sentence='Φ' # mark to distinguish the first letter of a sentence\n",
    "                continue\n",
    "            tag = line.split(\"\\t\")[2].strip('\\n')\n",
    "            sentence = sentence + ' ' + tag\n",
    "            \n",
    "    return for_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a transition dictionary\n",
    "def make_transition(tag, for_transition):\n",
    "    transition = {}\n",
    "    for pre, pre_cnt in tag.items(): \n",
    "        for cur, cur_cnt in tag.items():\n",
    "            tot = 0   \n",
    "            for sentence in for_transition: # Find 'tag tag' in each sentence\n",
    "                find = pre + ' ' + cur\n",
    "                if find in sentence: # Count(s->s')\n",
    "                    tot += 1\n",
    "            \n",
    "            transition[str((pre,cur))] = tot/pre_cnt # Calculate t(s'|s)\n",
    "            \n",
    "    return transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_transition = make_sentences_for_transition('./data/train')\n",
    "tag['Φ'] = len(for_transition) # Insert initial tag into the tag dictionary\n",
    "transition = make_transition(tag, for_transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many transition parameters in my HMM? : 2116\n"
     ]
    }
   ],
   "source": [
    "print('How many transition parameters in my HMM? : %d'%len(transition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca = [x[0] for x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sentences for emission\n",
    "def make_sentences_for_emission(file):\n",
    "    with open(file, \"r\") as train:\n",
    "        for_emission = {}\n",
    "        for line in train:\n",
    "            if not line.split(): # Ignore a blank line\n",
    "                continue\n",
    "            word, wordtag = line.split(\"\\t\")[1], line.split(\"\\t\")[2].strip('\\n')\n",
    "            if isNumber(word): # number to '<num>' token\n",
    "                word = '<num>'\n",
    "            else:\n",
    "                if word not in voca:\n",
    "                    word = unk_preprocessing(word)\n",
    "            \n",
    "            x = word + ' ' + wordtag # Count(s->x)\n",
    "            if x not in for_emission:\n",
    "                for_emission[x] = 1\n",
    "            else:\n",
    "                for_emission[x] += 1\n",
    "                \n",
    "    return for_emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a emission dictionary\n",
    "def make_emission(tag, for_emission):\n",
    "    emission={}\n",
    "    for comb, cnt in for_emission.items():\n",
    "        eword = comb.split(\" \")[0]\n",
    "        etag = comb.split(\" \")[1]\n",
    "\n",
    "        emission[str((etag,eword))] = cnt / tag[etag] # calculate e(x|s)\n",
    "        \n",
    "    return emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_emission = make_sentences_for_emission('./data/train')\n",
    "emission = make_emission(tag, for_emission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many emission parameters in my HMM? : 28754\n"
     ]
    }
   ],
   "source": [
    "print('How many emission parameters in my HMM? : %d'%len(emission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a hmm.json file\n",
    "import json\n",
    "\n",
    "hmm = {}\n",
    "hmm['transition'] = transition\n",
    "hmm['emission'] = emission\n",
    "\n",
    "with open('hmm.json', 'w') as outfile:\n",
    "    json.dump(hmm, outfile, indent='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3 : Greedy Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy algorithm\n",
    "def greedy(file, tag):\n",
    "    state = []\n",
    "    pre = 'Φ' # To distinguosh sentence, use this mark. 'Φ' is the first letter of the sentence\n",
    "    with open(file, \"r\") as train:\n",
    "        sentence = []\n",
    "        for line in train:\n",
    "            p = []\n",
    "            if not line.split(): \n",
    "                pre='Φ'\n",
    "                continue\n",
    "                \n",
    "            word = line.split(\"\\t\")[1].strip('\\n')\n",
    "            if isNumber(word): # change number to '<num>' token\n",
    "                word = '<num>'\n",
    "            else:\n",
    "                if word not in voca:\n",
    "                    word = unk_preprocessing(word)\n",
    "            for cur in list(tag.keys()):\n",
    "                emission_val = 0 if str((cur,word)) not in emission else emission[str((cur,word))]\n",
    "                prob = emission_val * transition[str((pre,cur))] # calculate probabilities\n",
    "                p.append(prob)\n",
    "            \n",
    "            state_max = list(tag.keys())[p.index(max(p))] \n",
    "            state.append(state_max)    # put tag to the'state' list\n",
    "            pre = state_max\n",
    "            \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = greedy('./data/dev',tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate greedy decoding\n",
    "def greedy_evaluate(file,state):\n",
    "    with open(file, \"r\") as train:\n",
    "        taglist = []\n",
    "        for line in train:\n",
    "            if not line.split():\n",
    "                continue\n",
    "            tag = line.split(\"\\t\")[2].strip('\\n')\n",
    "            taglist.append(tag)\n",
    "    \n",
    "    # compare the predicted tag and real tag\n",
    "    cnt=0\n",
    "    for greedy, real in zip(state, taglist): \n",
    "        if greedy == real:\n",
    "            cnt += 1\n",
    "\n",
    "    accuracy = (cnt / len(state)) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate greedy decoding\n",
    "def greedy_evaluate(file,state):\n",
    "    with open(file, \"r\") as train:\n",
    "        taglist = []\n",
    "        wordlist=[]\n",
    "        for line in train:\n",
    "            if not line.split():\n",
    "                continue\n",
    "            word = line.split(\"\\t\")[1] \n",
    "            tag = line.split(\"\\t\")[2].strip('\\n')\n",
    "            taglist.append(tag)\n",
    "            wordlist.append(word)\n",
    "    \n",
    "    # compare the predicted tag and real tag\n",
    "    cnt=0\n",
    "    for greedy, real, word in zip(state, taglist, wordlist): \n",
    "        if greedy == real:\n",
    "            cnt += 1\n",
    "\n",
    "    accuracy = (cnt / len(state)) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Algorithm's Accuracy =93.88\n"
     ]
    }
   ],
   "source": [
    "accuracy = greedy_evaluate('./data/dev',state)\n",
    "print(\"Greedy Algorithm's Accuracy =%0.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag = greedy('./data/test',tag) # predict test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sentence using test_tag\n",
    "write=[]\n",
    "with open('./data/test', \"r\") as test:\n",
    "    sentence=''\n",
    "    cnt = 0\n",
    "    for line in test:\n",
    "        if not line.split():\n",
    "            write.append('*****')\n",
    "            continue        \n",
    "        index, word = line.split(\"\\t\")[0], line.split(\"\\t\")[1].strip('\\n')\n",
    "        \n",
    "        pred_tag = test_tag[cnt]\n",
    "        sentence = index + '\\t' + word + '\\t' + pred_tag + '\\n'\n",
    "        write.append(sentence)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result in a file named 'greedy.out'\n",
    "with open('greedy.out', \"w\") as out:\n",
    "    for line in write:\n",
    "        if line == '*****':\n",
    "            out.write('\\n')\n",
    "        else:\n",
    "            out.write(line)  \n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task4 : Viterbi Decoding with HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(file, tag):\n",
    "    with open(file, \"r\") as train:\n",
    "        dic = []\n",
    "        state = []\n",
    "        sentence = []\n",
    "        one_sentence = []\n",
    "        # Make sentences to calculate conveniently\n",
    "        for line in train:\n",
    "            if not line.split():\n",
    "                sentence.append(one_sentence)\n",
    "                one_sentence = []\n",
    "                continue\n",
    "            word = line.split(\"\\t\")[1].strip('\\n')\n",
    "            if isNumber(word):\n",
    "                word = '<num>'\n",
    "            else:\n",
    "                if word not in voca:\n",
    "                    word = unk_preprocessing(word)\n",
    "            one_sentence.append(word)\n",
    "        sentence.append(one_sentence)\n",
    "        \n",
    "        # Calculate the first word of the sentence\n",
    "        for idx, s in enumerate(sentence):\n",
    "            Viterbi = [{}]\n",
    "            for st in list(tag.keys())[:-1]:\n",
    "                transition_val = transition[str(('Φ',st))]\n",
    "                #If word does not exist in the train data, assign a very low probability of 0.0000000000001\n",
    "                emission_val =  0.0000000001 if str((st,sentence[idx][0])) not in emission else emission[str((st,sentence[idx][0]))]\n",
    "                Viterbi[0][st] = {\"prob\": transition_val * emission_val, \"prev\": None}\n",
    "               \n",
    "            \n",
    "            # Run Viterbi algorithm with the remain of the sentence\n",
    "            for t in range(1, len(sentence[idx])):\n",
    "                Viterbi.append({})\n",
    "                states = list(tag.keys())[:-1]\n",
    "                for st in states:\n",
    "                    max_transition_prob = Viterbi[t - 1][states[0]][\"prob\"] * transition[str((states[0],st))] # max transition prop\n",
    "                    previous_selected = states[0]\n",
    "                    for prev in states[1:]:\n",
    "                        transition_prob = Viterbi[t - 1][prev][\"prob\"] * transition[str((prev,st))]\n",
    "                        if transition_prob > max_transition_prob:\n",
    "                            max_transition_prob = transition_prob\n",
    "                            previous_selected = prev\n",
    "                    \n",
    "                    #If word does not exist in the train data, assign a very low probability of 0.0000000000001\n",
    "                    emission_val = 0.0000000001 if str((st,sentence[idx][t])) not in emission else emission[str((st,sentence[idx][t]))]                        \n",
    "                    max_prob = max_transition_prob * emission_val\n",
    "                    Viterbi[t][st] = {\"prob\": max_prob, \"prev\": previous_selected}\n",
    "                    \n",
    "            predicted = []\n",
    "            max_prob = 0\n",
    "            best_st = None\n",
    "            \n",
    "            # FInd most highest probabilities and save it\n",
    "            for st, item in Viterbi[-1].items():\n",
    "                if item[\"prob\"] > max_prob:\n",
    "                    max_prob = item[\"prob\"]\n",
    "                    best = st\n",
    "            predicted.append(best)\n",
    "            previous = best\n",
    "            \n",
    "            # backtracking\n",
    "            for t in range(len(Viterbi) - 2, -1, -1):\n",
    "                predicted.insert(0, Viterbi[t + 1][previous][\"prev\"])\n",
    "                previous = Viterbi[t + 1][previous][\"prev\"]\n",
    "\n",
    "            dic.append(predicted)\n",
    "            \n",
    "        return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dic = viterbi('./data/dev',tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def viterbi_evaluate(file,dic):\n",
    "    with open(file, \"r\") as train:\n",
    "        taglist = []\n",
    "        one_sentence =[]\n",
    "        # make a sentence that consists of the tags\n",
    "        for line in train:\n",
    "            if not line.split():\n",
    "                taglist.append(one_sentence)\n",
    "                one_sentence = []\n",
    "                continue\n",
    "            tag = line.split(\"\\t\")[2].strip('\\n')\n",
    "            one_sentence.append(tag)\n",
    "        taglist.append(one_sentence)\n",
    "      \n",
    "    tot_cnt=0\n",
    "    same_cnt=0\n",
    "    for i in range(len(taglist)):\n",
    "        list_len = len(dic[i])\n",
    "        tot_cnt += list_len    \n",
    "        for k in range(list_len):\n",
    "            if dic[i][k] == taglist[i][k]:\n",
    "                same_cnt += 1      \n",
    "\n",
    "    accuracy = (same_cnt / tot_cnt) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi Algorithm's Accuracy =94.84\n"
     ]
    }
   ],
   "source": [
    "accuracy = viterbi_evaluate('./data/dev',dic)\n",
    "print(\"Viterbi Algorithm's Accuracy =%0.2f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dic = viterbi('./data/test',tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. for storing the result in the file - make sentences\n",
    "write=[]\n",
    "for i in range(len(test_dic)):\n",
    "    for j in range(len(test_dic[i])):\n",
    "        write.append(test_dic[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. for storing the result in the file - make a same format of training data\n",
    "write2=[]\n",
    "with open('./data/test', \"r\") as test:\n",
    "    sentence=''\n",
    "    cnt = 0\n",
    "    for line in test:\n",
    "        if not line.split():\n",
    "            write2.append('*****')\n",
    "            continue        \n",
    "        index, word = line.split(\"\\t\")[0], line.split(\"\\t\")[1].strip('\\n')\n",
    "        \n",
    "        pred_tag = write[cnt]\n",
    "        sentence = index + '\\t' + word + '\\t' + pred_tag + '\\n'\n",
    "        write2.append(sentence)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for storing the result in the file - store data\n",
    "with open('viterbi.out', \"w\") as out:\n",
    "    for line in write2:\n",
    "        if line == '*****':\n",
    "            out.write('\\n')\n",
    "        else:\n",
    "            out.write(line)  \n",
    "    out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
