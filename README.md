# Applied-Natural-Language-Processing @USC

Grade : A

1. <b> Text classification for sentiment analysis with amazon review </b> : Preprocessing / Feature Extraction / Perceptron / SVM / Logistic Regression / Multinomial Naive Bayes

2. <b> Word Embedding </b> : TF-IDF, pretrained Word2Vec, my own Word2Vec) / Simple models / Feedforward Neural Networks / Recurrent Neural Networks (Simple RNN, GRU)

3. <b> Build an HMM model using Greedy Algorithm and Viterbi Algorithm for part-of-speech tagging </b> : Improved each modelâ€™s accuracy to 93.8%(Greedy) and 94.8%(Viterbi) by subdividing unknown words based on characteristics of numbers, nouns, verbs, adjectives, and adverbs.

4. <b> Build Name Entitiy Recognition Model using BLSTM, GloVe word embedding and SGD optimizer. </b> : Increased the overall accuracy to 98.2% and overall F1 score to 88% on general model by classifying unknown words based on characteristics of numbers, nouns, verbs, adjectives, and adverbs.

5. <b> Long Document Summarization </b> : Detected and analyzed pain points of document summarization regarding length and different domains by applying extractive summarization model to long government documents. Evaluated model results using ROUGE and human evaluation and conducted error analysis by comparing the model to baseline model and ground truth summaries.

