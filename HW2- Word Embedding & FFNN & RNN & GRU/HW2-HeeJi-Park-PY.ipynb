{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hee Ji Park (4090715830)\n",
    "\n",
    "- CSCI544 : Homewoek Assignment 2\n",
    "- Python version: 3.6.12\n",
    "- Jupyter notebook version : 6.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation\n",
    "#### (0) import package and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gmlwl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score # For the result\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # For TF-IDF\n",
    "from sklearn.linear_model import Perceptron # For perceptron\n",
    "from sklearn.svm import LinearSVC # For SVM\n",
    "# For data cleaning and preprocessing\n",
    "from nltk.corpus import stopwords # remove the stop words using NLTK package\n",
    "import contractions \n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "# For Word2Vec\n",
    "from gensim.test.utils import common_texts \n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# For FNN and RNN\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 16148: expected 15 fields, saw 22\\nSkipping line 20100: expected 15 fields, saw 22\\nSkipping line 45178: expected 15 fields, saw 22\\nSkipping line 48700: expected 15 fields, saw 22\\nSkipping line 63331: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 86053: expected 15 fields, saw 22\\nSkipping line 88858: expected 15 fields, saw 22\\nSkipping line 115017: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 137366: expected 15 fields, saw 22\\nSkipping line 139110: expected 15 fields, saw 22\\nSkipping line 165540: expected 15 fields, saw 22\\nSkipping line 171813: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 203723: expected 15 fields, saw 22\\nSkipping line 209366: expected 15 fields, saw 22\\nSkipping line 211310: expected 15 fields, saw 22\\nSkipping line 246351: expected 15 fields, saw 22\\nSkipping line 252364: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 267003: expected 15 fields, saw 22\\nSkipping line 268957: expected 15 fields, saw 22\\nSkipping line 303336: expected 15 fields, saw 22\\nSkipping line 306021: expected 15 fields, saw 22\\nSkipping line 311569: expected 15 fields, saw 22\\nSkipping line 316767: expected 15 fields, saw 22\\nSkipping line 324009: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 359107: expected 15 fields, saw 22\\nSkipping line 368367: expected 15 fields, saw 22\\nSkipping line 381180: expected 15 fields, saw 22\\nSkipping line 390453: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 412243: expected 15 fields, saw 22\\nSkipping line 419342: expected 15 fields, saw 22\\nSkipping line 457388: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 459935: expected 15 fields, saw 22\\nSkipping line 460167: expected 15 fields, saw 22\\nSkipping line 466460: expected 15 fields, saw 22\\nSkipping line 500314: expected 15 fields, saw 22\\nSkipping line 500339: expected 15 fields, saw 22\\nSkipping line 505396: expected 15 fields, saw 22\\nSkipping line 507760: expected 15 fields, saw 22\\nSkipping line 513626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 527638: expected 15 fields, saw 22\\nSkipping line 534209: expected 15 fields, saw 22\\nSkipping line 535687: expected 15 fields, saw 22\\nSkipping line 547671: expected 15 fields, saw 22\\nSkipping line 549054: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 599929: expected 15 fields, saw 22\\nSkipping line 604776: expected 15 fields, saw 22\\nSkipping line 609937: expected 15 fields, saw 22\\nSkipping line 632059: expected 15 fields, saw 22\\nSkipping line 638546: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 665017: expected 15 fields, saw 22\\nSkipping line 677680: expected 15 fields, saw 22\\nSkipping line 684370: expected 15 fields, saw 22\\nSkipping line 720217: expected 15 fields, saw 29\\n'\n",
      "b'Skipping line 723240: expected 15 fields, saw 22\\nSkipping line 723433: expected 15 fields, saw 22\\nSkipping line 763891: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 800288: expected 15 fields, saw 22\\nSkipping line 802942: expected 15 fields, saw 22\\nSkipping line 803379: expected 15 fields, saw 22\\nSkipping line 805122: expected 15 fields, saw 22\\nSkipping line 821899: expected 15 fields, saw 22\\nSkipping line 831707: expected 15 fields, saw 22\\nSkipping line 842829: expected 15 fields, saw 22\\nSkipping line 843604: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 863904: expected 15 fields, saw 22\\nSkipping line 875655: expected 15 fields, saw 22\\nSkipping line 886796: expected 15 fields, saw 22\\nSkipping line 892299: expected 15 fields, saw 22\\nSkipping line 902518: expected 15 fields, saw 22\\nSkipping line 903079: expected 15 fields, saw 22\\nSkipping line 912678: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 932953: expected 15 fields, saw 22\\nSkipping line 936838: expected 15 fields, saw 22\\nSkipping line 937177: expected 15 fields, saw 22\\nSkipping line 947695: expected 15 fields, saw 22\\nSkipping line 960713: expected 15 fields, saw 22\\nSkipping line 965225: expected 15 fields, saw 22\\nSkipping line 980776: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 999318: expected 15 fields, saw 22\\nSkipping line 1007247: expected 15 fields, saw 22\\nSkipping line 1015987: expected 15 fields, saw 22\\nSkipping line 1018984: expected 15 fields, saw 22\\nSkipping line 1028671: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1063360: expected 15 fields, saw 22\\nSkipping line 1066195: expected 15 fields, saw 22\\nSkipping line 1066578: expected 15 fields, saw 22\\nSkipping line 1066869: expected 15 fields, saw 22\\nSkipping line 1068809: expected 15 fields, saw 22\\nSkipping line 1069505: expected 15 fields, saw 22\\nSkipping line 1087983: expected 15 fields, saw 22\\nSkipping line 1108184: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1118137: expected 15 fields, saw 22\\nSkipping line 1142723: expected 15 fields, saw 22\\nSkipping line 1152492: expected 15 fields, saw 22\\nSkipping line 1156947: expected 15 fields, saw 22\\nSkipping line 1172563: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1209254: expected 15 fields, saw 22\\nSkipping line 1212966: expected 15 fields, saw 22\\nSkipping line 1236533: expected 15 fields, saw 22\\nSkipping line 1237598: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1273825: expected 15 fields, saw 22\\nSkipping line 1277898: expected 15 fields, saw 22\\nSkipping line 1283654: expected 15 fields, saw 22\\nSkipping line 1286023: expected 15 fields, saw 22\\nSkipping line 1302038: expected 15 fields, saw 22\\nSkipping line 1305179: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1326022: expected 15 fields, saw 22\\nSkipping line 1338120: expected 15 fields, saw 22\\nSkipping line 1338503: expected 15 fields, saw 22\\nSkipping line 1338849: expected 15 fields, saw 22\\nSkipping line 1341513: expected 15 fields, saw 22\\nSkipping line 1346493: expected 15 fields, saw 22\\nSkipping line 1373127: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1389508: expected 15 fields, saw 22\\nSkipping line 1413951: expected 15 fields, saw 22\\nSkipping line 1433626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1442698: expected 15 fields, saw 22\\nSkipping line 1472982: expected 15 fields, saw 22\\nSkipping line 1482282: expected 15 fields, saw 22\\nSkipping line 1487808: expected 15 fields, saw 22\\nSkipping line 1500636: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1511479: expected 15 fields, saw 22\\nSkipping line 1532302: expected 15 fields, saw 22\\nSkipping line 1537952: expected 15 fields, saw 22\\nSkipping line 1539951: expected 15 fields, saw 22\\nSkipping line 1541020: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1594217: expected 15 fields, saw 22\\nSkipping line 1612264: expected 15 fields, saw 22\\nSkipping line 1615907: expected 15 fields, saw 22\\nSkipping line 1621859: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1653542: expected 15 fields, saw 22\\nSkipping line 1671537: expected 15 fields, saw 22\\nSkipping line 1672879: expected 15 fields, saw 22\\nSkipping line 1674523: expected 15 fields, saw 22\\nSkipping line 1677355: expected 15 fields, saw 22\\nSkipping line 1703907: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1713046: expected 15 fields, saw 22\\nSkipping line 1722982: expected 15 fields, saw 22\\nSkipping line 1727290: expected 15 fields, saw 22\\nSkipping line 1744482: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1803858: expected 15 fields, saw 22\\nSkipping line 1810069: expected 15 fields, saw 22\\nSkipping line 1829751: expected 15 fields, saw 22\\nSkipping line 1831699: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1863131: expected 15 fields, saw 22\\nSkipping line 1867917: expected 15 fields, saw 22\\nSkipping line 1874790: expected 15 fields, saw 22\\nSkipping line 1879952: expected 15 fields, saw 22\\nSkipping line 1880501: expected 15 fields, saw 22\\nSkipping line 1886655: expected 15 fields, saw 22\\nSkipping line 1887888: expected 15 fields, saw 22\\nSkipping line 1894286: expected 15 fields, saw 22\\nSkipping line 1895400: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1904040: expected 15 fields, saw 22\\nSkipping line 1907604: expected 15 fields, saw 22\\nSkipping line 1915739: expected 15 fields, saw 22\\nSkipping line 1921514: expected 15 fields, saw 22\\nSkipping line 1939428: expected 15 fields, saw 22\\nSkipping line 1944342: expected 15 fields, saw 22\\nSkipping line 1949699: expected 15 fields, saw 22\\nSkipping line 1961872: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1968846: expected 15 fields, saw 22\\nSkipping line 1999941: expected 15 fields, saw 22\\nSkipping line 2001492: expected 15 fields, saw 22\\nSkipping line 2011204: expected 15 fields, saw 22\\nSkipping line 2025295: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2041266: expected 15 fields, saw 22\\nSkipping line 2073314: expected 15 fields, saw 22\\nSkipping line 2080133: expected 15 fields, saw 22\\nSkipping line 2088521: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2103490: expected 15 fields, saw 22\\nSkipping line 2115278: expected 15 fields, saw 22\\nSkipping line 2153174: expected 15 fields, saw 22\\nSkipping line 2161731: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2165250: expected 15 fields, saw 22\\nSkipping line 2175132: expected 15 fields, saw 22\\nSkipping line 2206817: expected 15 fields, saw 22\\nSkipping line 2215848: expected 15 fields, saw 22\\nSkipping line 2223811: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2257265: expected 15 fields, saw 22\\nSkipping line 2259163: expected 15 fields, saw 22\\nSkipping line 2263291: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2301943: expected 15 fields, saw 22\\nSkipping line 2304371: expected 15 fields, saw 22\\nSkipping line 2306015: expected 15 fields, saw 22\\nSkipping line 2312186: expected 15 fields, saw 22\\nSkipping line 2314740: expected 15 fields, saw 22\\nSkipping line 2317754: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2383514: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2449763: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2589323: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2775036: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2935174: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3078830: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3123091: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3185533: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4150395: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4748401: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>37000337</td>\n",
       "      <td>R3DT59XH7HXR9K</td>\n",
       "      <td>B00303FI0G</td>\n",
       "      <td>529320574</td>\n",
       "      <td>Arthur Court Paper Towel Holder</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Beautiful. Looks great on counter</td>\n",
       "      <td>Beautiful.  Looks great on counter.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>15272914</td>\n",
       "      <td>R1LFS11BNASSU8</td>\n",
       "      <td>B00JCZKZN6</td>\n",
       "      <td>274237558</td>\n",
       "      <td>Olde Thompson Bavaria Glass Salt and Pepper Mi...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Awesome &amp; Self-ness</td>\n",
       "      <td>I personally have 5 days sets and have also bo...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>36137863</td>\n",
       "      <td>R296RT05AG0AF6</td>\n",
       "      <td>B00JLIKA5C</td>\n",
       "      <td>544675303</td>\n",
       "      <td>Progressive International PL8 Professional Man...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Fabulous and worth every penny</td>\n",
       "      <td>Fabulous and worth every penny. Used for clean...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>43311049</td>\n",
       "      <td>R3V37XDZ7ZCI3L</td>\n",
       "      <td>B000GBNB8G</td>\n",
       "      <td>491599489</td>\n",
       "      <td>Zyliss Jumbo Garlic Press</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>A must if you love garlic on tomato marinara s...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>13763148</td>\n",
       "      <td>R14GU232NQFYX2</td>\n",
       "      <td>B00VJ5KX9S</td>\n",
       "      <td>353790155</td>\n",
       "      <td>1 X Premier Pizza Cutter - Stainless Steel 14\"...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Better than sex</td>\n",
       "      <td>Worth every penny! Buy one now and be a pizza ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     37000337  R3DT59XH7HXR9K  B00303FI0G       529320574   \n",
       "1          US     15272914  R1LFS11BNASSU8  B00JCZKZN6       274237558   \n",
       "2          US     36137863  R296RT05AG0AF6  B00JLIKA5C       544675303   \n",
       "3          US     43311049  R3V37XDZ7ZCI3L  B000GBNB8G       491599489   \n",
       "4          US     13763148  R14GU232NQFYX2  B00VJ5KX9S       353790155   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0                    Arthur Court Paper Towel Holder          Kitchen   \n",
       "1  Olde Thompson Bavaria Glass Salt and Pepper Mi...          Kitchen   \n",
       "2  Progressive International PL8 Professional Man...          Kitchen   \n",
       "3                          Zyliss Jumbo Garlic Press          Kitchen   \n",
       "4  1 X Premier Pizza Cutter - Stainless Steel 14\"...          Kitchen   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0          5.0            0.0          0.0    N                 Y   \n",
       "1          5.0            0.0          1.0    N                 Y   \n",
       "2          5.0            0.0          0.0    N                 Y   \n",
       "3          5.0            0.0          1.0    N                 Y   \n",
       "4          5.0            0.0          0.0    N                 Y   \n",
       "\n",
       "                     review_headline  \\\n",
       "0  Beautiful. Looks great on counter   \n",
       "1                Awesome & Self-ness   \n",
       "2     Fabulous and worth every penny   \n",
       "3                         Five Stars   \n",
       "4                    Better than sex   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                Beautiful.  Looks great on counter.  2015-08-31  \n",
       "1  I personally have 5 days sets and have also bo...  2015-08-31  \n",
       "2  Fabulous and worth every penny. Used for clean...  2015-08-31  \n",
       "3  A must if you love garlic on tomato marinara s...  2015-08-31  \n",
       "4  Worth every penny! Buy one now and be a pizza ...  2015-08-31  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.tsv', sep='\\t', error_bad_lines=False) # read data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Build a bananced dataset of 250k reviews along with their rationg(50K per instances per each rating score) through random selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  1.,  3.,  4.,  2., nan])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the value of the star_rating \n",
    "df['star_rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove strange star_rating values like 'nan'\n",
    "df.drop(df[df['star_rating'].isna()].index, inplace = True)\n",
    "\n",
    "# change float type to int type\n",
    "df['star_rating'] = df['star_rating'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset along with their rationg(50K per instances per each rating score) through random selection\n",
    "sample1 = df[df.star_rating == 1].sample(n=5000, random_state=2)\n",
    "sample2 = df[df.star_rating == 2].sample(n=5000, random_state=2)\n",
    "sample3 = df[df.star_rating == 3].sample(n=5000, random_state=2)\n",
    "sample4 = df[df.star_rating == 4].sample(n=5000, random_state=2)\n",
    "sample5 = df[df.star_rating == 5].sample(n=5000, random_state=2)\n",
    "\n",
    "# concatenate sample 1-5 as a new dataframe called 'new'\n",
    "new = sample1.append([sample2,sample3,sample4,sample5]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 5000, 4: 5000, 3: 5000, 2: 5000, 1: 5000}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if each grade(star_rating) has 50K instances\n",
    "rating_count = {k: v for k, v in zip(new['star_rating'].value_counts().index, new['star_rating'].value_counts())}\n",
    "rating_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Create ternary labels using the ratings\n",
    "\n",
    "- class 1 : rating = 4 or 5\n",
    "- class 2 : rating = 1 or 2\n",
    "- class 3 : rating = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create ternary labels, mapping the ratings.\n",
    "new.loc[(new['star_rating'] > 3), 'class'] = 1\n",
    "new.loc[(new['star_rating'] < 3), 'class'] = 2\n",
    "new.loc[(new['star_rating'] == 3), 'class'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>11903534</td>\n",
       "      <td>R3NBYY1PF9MXRT</td>\n",
       "      <td>B00FRMV38Y</td>\n",
       "      <td>872686823</td>\n",
       "      <td>Decodyne&amp;#0153; Morning Mug, Heat Sensitive Co...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Didn't deliver</td>\n",
       "      <td>Really didn't work as well as the pictures. Wh...</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>27885863</td>\n",
       "      <td>R1A10GP8CPG1A2</td>\n",
       "      <td>B003YFI0O6</td>\n",
       "      <td>111524501</td>\n",
       "      <td>Oster Electric Wine-Bottle Opener</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>didn't work even the first time</td>\n",
       "      <td>brand new out of the box and it wouldn't even ...</td>\n",
       "      <td>2014-03-21</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>15355157</td>\n",
       "      <td>R147NFWDLR0AT9</td>\n",
       "      <td>B0002T4ZL4</td>\n",
       "      <td>978772977</td>\n",
       "      <td>Oggi 5355 4-Piece Acrylic Canister Set with Ai...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>15647704</td>\n",
       "      <td>R1GQQLPV9LCY1T</td>\n",
       "      <td>B0034J6QIY</td>\n",
       "      <td>591197834</td>\n",
       "      <td>Cuisinart SS-700 Single Serve Brewing System -...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Cuisinart Keurig ruined my week!</td>\n",
       "      <td>Really bummed! Machine worked great until a fe...</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26424346</td>\n",
       "      <td>R1X5BB0UPZ4IWT</td>\n",
       "      <td>B000AXQA8I</td>\n",
       "      <td>330600737</td>\n",
       "      <td>Kuhn Rikon Twist and Chop, Artichoke</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Not what I was expecting</td>\n",
       "      <td>Sure, it cuts things, but the blades don't hav...</td>\n",
       "      <td>2007-02-20</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     11903534  R3NBYY1PF9MXRT  B00FRMV38Y       872686823   \n",
       "1          US     27885863  R1A10GP8CPG1A2  B003YFI0O6       111524501   \n",
       "2          US     15355157  R147NFWDLR0AT9  B0002T4ZL4       978772977   \n",
       "3          US     15647704  R1GQQLPV9LCY1T  B0034J6QIY       591197834   \n",
       "4          US     26424346  R1X5BB0UPZ4IWT  B000AXQA8I       330600737   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  Decodyne&#0153; Morning Mug, Heat Sensitive Co...          Kitchen   \n",
       "1                  Oster Electric Wine-Bottle Opener          Kitchen   \n",
       "2  Oggi 5355 4-Piece Acrylic Canister Set with Ai...          Kitchen   \n",
       "3  Cuisinart SS-700 Single Serve Brewing System -...          Kitchen   \n",
       "4               Kuhn Rikon Twist and Chop, Artichoke          Kitchen   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            1            0.0          0.0    N                 Y   \n",
       "1            1            1.0          1.0    N                 Y   \n",
       "2            1            2.0          2.0    N                 Y   \n",
       "3            1            0.0          1.0    N                 Y   \n",
       "4            1            3.0          4.0    N                 N   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                     Didn't deliver   \n",
       "1                    didn't work even the first time   \n",
       "2  pure junk, lowest quality you could possibly g...   \n",
       "3                   Cuisinart Keurig ruined my week!   \n",
       "4                           Not what I was expecting   \n",
       "\n",
       "                                         review_body review_date  class  \n",
       "0  Really didn't work as well as the pictures. Wh...  2014-12-01    2.0  \n",
       "1  brand new out of the box and it wouldn't even ...  2014-03-21    2.0  \n",
       "2  pure junk, lowest quality you could possibly g...  2015-07-07    2.0  \n",
       "3  Really bummed! Machine worked great until a fe...  2015-02-28    2.0  \n",
       "4  Sure, it cuts things, but the blades don't hav...  2007-02-20    2.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Store dataset after generation and reuse ot to reduce the computational load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>11903534</td>\n",
       "      <td>R3NBYY1PF9MXRT</td>\n",
       "      <td>B00FRMV38Y</td>\n",
       "      <td>872686823</td>\n",
       "      <td>Decodyne&amp;#0153; Morning Mug, Heat Sensitive Co...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Didn't deliver</td>\n",
       "      <td>Really didn't work as well as the pictures. Wh...</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>27885863</td>\n",
       "      <td>R1A10GP8CPG1A2</td>\n",
       "      <td>B003YFI0O6</td>\n",
       "      <td>111524501</td>\n",
       "      <td>Oster Electric Wine-Bottle Opener</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>didn't work even the first time</td>\n",
       "      <td>brand new out of the box and it wouldn't even ...</td>\n",
       "      <td>2014-03-21</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>15355157</td>\n",
       "      <td>R147NFWDLR0AT9</td>\n",
       "      <td>B0002T4ZL4</td>\n",
       "      <td>978772977</td>\n",
       "      <td>Oggi 5355 4-Piece Acrylic Canister Set with Ai...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>15647704</td>\n",
       "      <td>R1GQQLPV9LCY1T</td>\n",
       "      <td>B0034J6QIY</td>\n",
       "      <td>591197834</td>\n",
       "      <td>Cuisinart SS-700 Single Serve Brewing System -...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Cuisinart Keurig ruined my week!</td>\n",
       "      <td>Really bummed! Machine worked great until a fe...</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26424346</td>\n",
       "      <td>R1X5BB0UPZ4IWT</td>\n",
       "      <td>B000AXQA8I</td>\n",
       "      <td>330600737</td>\n",
       "      <td>Kuhn Rikon Twist and Chop, Artichoke</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Not what I was expecting</td>\n",
       "      <td>Sure, it cuts things, but the blades don't hav...</td>\n",
       "      <td>2007-02-20</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     11903534  R3NBYY1PF9MXRT  B00FRMV38Y       872686823   \n",
       "1          US     27885863  R1A10GP8CPG1A2  B003YFI0O6       111524501   \n",
       "2          US     15355157  R147NFWDLR0AT9  B0002T4ZL4       978772977   \n",
       "3          US     15647704  R1GQQLPV9LCY1T  B0034J6QIY       591197834   \n",
       "4          US     26424346  R1X5BB0UPZ4IWT  B000AXQA8I       330600737   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  Decodyne&#0153; Morning Mug, Heat Sensitive Co...          Kitchen   \n",
       "1                  Oster Electric Wine-Bottle Opener          Kitchen   \n",
       "2  Oggi 5355 4-Piece Acrylic Canister Set with Ai...          Kitchen   \n",
       "3  Cuisinart SS-700 Single Serve Brewing System -...          Kitchen   \n",
       "4               Kuhn Rikon Twist and Chop, Artichoke          Kitchen   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            1            0.0          0.0    N                 Y   \n",
       "1            1            1.0          1.0    N                 Y   \n",
       "2            1            2.0          2.0    N                 Y   \n",
       "3            1            0.0          1.0    N                 Y   \n",
       "4            1            3.0          4.0    N                 N   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                     Didn't deliver   \n",
       "1                    didn't work even the first time   \n",
       "2  pure junk, lowest quality you could possibly g...   \n",
       "3                   Cuisinart Keurig ruined my week!   \n",
       "4                           Not what I was expecting   \n",
       "\n",
       "                                         review_body review_date  class  \n",
       "0  Really didn't work as well as the pictures. Wh...  2014-12-01    2.0  \n",
       "1  brand new out of the box and it wouldn't even ...  2014-03-21    2.0  \n",
       "2  pure junk, lowest quality you could possibly g...  2015-07-07    2.0  \n",
       "3  Really bummed! Machine worked great until a fe...  2015-02-28    2.0  \n",
       "4  Sure, it cuts things, but the blades don't hav...  2007-02-20    2.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.to_csv('ternary_data.csv') # ternary dataset\n",
    "new = pd.read_csv('ternary_data.csv', index_col=0)\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Split the dataset : 80% traning set and 20% testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(new['review_body'], new['class'], test_size=0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2. Word Embedding\n",
    "\n",
    "- reference source : https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "### (a)\n",
    "#### (1) Load the pretrained \"word2vec-google-news-300\" Word2Vec model and learn how to extract word embeddings for your datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Check semantic similarities of the generated vecotrs using two examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1) cat + dog = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('puppy', 0.8089798092842102)]\n",
      "[('puppy', 0.7729379534721375)]\n"
     ]
    }
   ],
   "source": [
    "# calculate the similarity using the default \"cosine similarity\" measure.\n",
    "print(wv.most_similar(positive=['cat','dog'], topn=1))\n",
    "\n",
    "# calculate the similarity using different measure \"cosmul\" \n",
    "print(wv.most_similar_cosmul(positive=['cat','dog'], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (2) Similarity between 'excellent' and 'outstanding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55674857\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('excellent','outstanding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> Similarity between 'excellent' and 'outstanding' is lower than I expected. \n",
    "So I'll check similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('terrific', 0.7409726977348328), ('superb', 0.7062715888023376), ('exceptional', 0.681470513343811), ('fantastic', 0.6802847981452942), ('good', 0.6442928910255432), ('great', 0.6124600172042847), ('Excellent', 0.6091997623443604), ('impeccable', 0.5980967283248901), ('exemplary', 0.5959650278091431), ('marvelous', 0.582928478717804)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar('excellent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('oustanding', 0.8012188673019409), ('Outstanding', 0.6041857600212097), ('exceptional', 0.6031844615936279), ('anchorman_Jason_Lezak', 0.5947381258010864), ('outsanding', 0.566262423992157), ('Stock_HEI', 0.5573362708091736), ('excellent', 0.556748628616333), ('Synplicity_FPGA_implementation', 0.5520347356796265), ('exemplary', 0.5467386245727539), ('W3_Awards_honors', 0.5172522068023682)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar('outstanding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (3) Similarity between 'chair' and 'desk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3149568\n"
     ]
    }
   ],
   "source": [
    "print(wv.similarity('chair','desk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (4) Similar things with soap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('soaps', 0.7613304257392883), ('Soap', 0.6950218677520752), ('Colgate_Palmolive_toothpaste', 0.6457595229148865), ('detergent', 0.5981624126434326), ('Colgate_toothpaste_Palmolive', 0.5842004418373108), ('antiseptic_soaps', 0.5792285203933716), ('soapy', 0.5768905878067017), ('Laundry_detergent', 0.5718933939933777), ('Tide_detergent_Ivory', 0.5658919215202332), ('Unilever_ULVR_LN', 0.5604559779167175)]\n"
     ]
    }
   ],
   "source": [
    "print(wv.most_similar('soap'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Train a Word2Vec model using my dataset\n",
    "- embedding size = 300\n",
    "- window size = 11\n",
    "- minimun word count = 10\n",
    "- sg = 1 : skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gmlwl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') # For tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(temp):\n",
    "    # For each sentence, word tokenization is performed using NLTK\n",
    "    result = [word_tokenize(sentence) for sentence in temp]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model using my own dataset\n",
    "# sg = 1 : skip-gram\n",
    "model = Word2Vec(sentences=tokenize(new['review_body']), vector_size=300, window=11, min_count=10, workers=4, sg=1)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Check the semantic similarities using my own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1) cat + dog = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('swear', 0.7196011543273926)]\n",
      "[('swear', 0.6660094261169434)]\n"
     ]
    }
   ],
   "source": [
    "# calculate the similarity using the default \"cosine similarity\" measure.\n",
    "print(model.wv.most_similar(positive=['cat','dog'], topn=1))\n",
    "\n",
    "# calculate the similarity using different measure \"cosmul\" \n",
    "print(model.wv.most_similar_cosmul(positive=['cat','dog'], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (2) Similarity between 'excellent' and 'outstanding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6124733\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('excellent','outstanding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('exceptional', 0.6363705992698669), ('outstanding', 0.6124733090400696), ('affordable', 0.6097689270973206), ('superb', 0.5697858333587646), ('acceptable', 0.5670941472053528), ('wonderful', 0.5664124488830566), ('adequate', 0.539593517780304), ('A+', 0.5354660749435425), ('awesome', 0.5353838205337524), ('amazingly', 0.5322791337966919)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('excellent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('speedy', 0.723319947719574), ('responsive', 0.7164787650108337), ('confirmed', 0.702231764793396), ('Seller', 0.7006707191467285), ('A+', 0.6945520043373108), ('exceptional', 0.6799882054328918), ('unacceptable', 0.6744035482406616), ('speaking', 0.6743783950805664), ('Company', 0.6710595488548279), ('Europe', 0.6699382066726685)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('outstanding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (3) Similarity between 'chair' and 'desk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48516065\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('chair','desk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (4) Similar things with soap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('soapy', 0.7130864858627319), ('detergent', 0.6908825635910034), ('sponge', 0.6703446507453918), ('dishwashing', 0.6624318361282349), ('wiping', 0.6428627967834473), ('bleach', 0.6349363327026367), ('rinsed', 0.6319710612297058), ('rinsing', 0.6148936748504639), ('scrubbed', 0.6108755469322205), ('deposits', 0.6097919344902039)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('soap'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Conclude) Comparing vectors generated by myself and the pretrained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Word2Vec1.PNG' height=50>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Conclude] What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "=> The Word2Vec model I trained uses Amazon review data, so there are many terms related to the product.\n",
    "Objectively, the Word2Vec model using Google News data is better to encode semantic similarities between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Simple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (0) First of all, we have to discard class 3(=rating 3).  Because we will only use class 1 and class 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>11903534</td>\n",
       "      <td>R3NBYY1PF9MXRT</td>\n",
       "      <td>B00FRMV38Y</td>\n",
       "      <td>872686823</td>\n",
       "      <td>Decodyne&amp;#0153; Morning Mug, Heat Sensitive Co...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Didn't deliver</td>\n",
       "      <td>Really didn't work as well as the pictures. Wh...</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>27885863</td>\n",
       "      <td>R1A10GP8CPG1A2</td>\n",
       "      <td>B003YFI0O6</td>\n",
       "      <td>111524501</td>\n",
       "      <td>Oster Electric Wine-Bottle Opener</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>didn't work even the first time</td>\n",
       "      <td>brand new out of the box and it wouldn't even ...</td>\n",
       "      <td>2014-03-21</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>15355157</td>\n",
       "      <td>R147NFWDLR0AT9</td>\n",
       "      <td>B0002T4ZL4</td>\n",
       "      <td>978772977</td>\n",
       "      <td>Oggi 5355 4-Piece Acrylic Canister Set with Ai...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>15647704</td>\n",
       "      <td>R1GQQLPV9LCY1T</td>\n",
       "      <td>B0034J6QIY</td>\n",
       "      <td>591197834</td>\n",
       "      <td>Cuisinart SS-700 Single Serve Brewing System -...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Cuisinart Keurig ruined my week!</td>\n",
       "      <td>Really bummed! Machine worked great until a fe...</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26424346</td>\n",
       "      <td>R1X5BB0UPZ4IWT</td>\n",
       "      <td>B000AXQA8I</td>\n",
       "      <td>330600737</td>\n",
       "      <td>Kuhn Rikon Twist and Chop, Artichoke</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Not what I was expecting</td>\n",
       "      <td>Sure, it cuts things, but the blades don't hav...</td>\n",
       "      <td>2007-02-20</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     11903534  R3NBYY1PF9MXRT  B00FRMV38Y       872686823   \n",
       "1          US     27885863  R1A10GP8CPG1A2  B003YFI0O6       111524501   \n",
       "2          US     15355157  R147NFWDLR0AT9  B0002T4ZL4       978772977   \n",
       "3          US     15647704  R1GQQLPV9LCY1T  B0034J6QIY       591197834   \n",
       "4          US     26424346  R1X5BB0UPZ4IWT  B000AXQA8I       330600737   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  Decodyne&#0153; Morning Mug, Heat Sensitive Co...          Kitchen   \n",
       "1                  Oster Electric Wine-Bottle Opener          Kitchen   \n",
       "2  Oggi 5355 4-Piece Acrylic Canister Set with Ai...          Kitchen   \n",
       "3  Cuisinart SS-700 Single Serve Brewing System -...          Kitchen   \n",
       "4               Kuhn Rikon Twist and Chop, Artichoke          Kitchen   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            1            0.0          0.0    N                 Y   \n",
       "1            1            1.0          1.0    N                 Y   \n",
       "2            1            2.0          2.0    N                 Y   \n",
       "3            1            0.0          1.0    N                 Y   \n",
       "4            1            3.0          4.0    N                 N   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                     Didn't deliver   \n",
       "1                    didn't work even the first time   \n",
       "2  pure junk, lowest quality you could possibly g...   \n",
       "3                   Cuisinart Keurig ruined my week!   \n",
       "4                           Not what I was expecting   \n",
       "\n",
       "                                         review_body review_date  class  \n",
       "0  Really didn't work as well as the pictures. Wh...  2014-12-01    2.0  \n",
       "1  brand new out of the box and it wouldn't even ...  2014-03-21    2.0  \n",
       "2  pure junk, lowest quality you could possibly g...  2015-07-07    2.0  \n",
       "3  Really bummed! Machine worked great until a fe...  2015-02-28    2.0  \n",
       "4  Sure, it cuts things, but the blades don't hav...  2007-02-20    2.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a new dataframe with class 1 and class 2.\n",
    "new = pd.read_csv('ternary_data.csv', index_col=0)\n",
    "new2 = new.loc[new['class'] < 3].reset_index(drop=True)\n",
    "new2.to_csv('binary_data.csv')\n",
    "new2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Really didn't work as well as the pictures. Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>brand new out of the box and it wouldn't even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Really bummed! Machine worked great until a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Sure, it cuts things, but the blades don't hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I bought a couple bambu bowls 3 months ago, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I bought this for my son. It arrived cracked, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>was impossible to use because did not fit any ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Flavors aren't good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The bristles are way too soft to move any silk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>After two months (and just after the return wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Was shipped fast and we received it on time. N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I absolutely would not buy or recommend this p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>This is the most poorly made item I have ever ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I bought the cookware set through Groupon and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>OK - I loved this machine at first but by opin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Like others who have the identical experience,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I received the package and when I opened it, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>While the picture and the description were a p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>We ordered two sets of these glasses and they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Never got it to work. Had to return it. The Ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The scale did not work when I received it.  In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Wanted to save a couple of dollars and bought ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I bought this slow cooker after reading the Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I returned it. It was very poorly made.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Makes very, very weak coffee when used, even w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Pure crap. Poorly made. Not worth the price by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Warning is on the packaging but nowhere on Ama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Barely gets warm. Hub part works fine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>It did great for a few weeks. The straw fell o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LOVE this gel paste!  I will be buying this fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Very happy with my purchase!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Makes an excellent, hot cup of coffee! Nice to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My husband had a curved paring knife in his ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Beautiful and practical!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>exactly what I needed to make fresh squeezed j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Brews coffee in minutes. Good size for two people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Love Mine so bought it for a friend as a gift.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OXO makes two different versions of this...ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>looks great and like that its a 16oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I was a skeptic at first but this is one of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This is a nice set of two salt or pepper mills...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ease of use, speed of cooking, (be sure to pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I've used this pan for years and could not fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Used this once, and it worked much better than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Theses favors are great quality and packaged v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I've been making my pies in my old pyrex dish....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I love these colorful mugs...it is exactly as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Good container&lt;br /&gt;Perfect Size&lt;br /&gt;Ordered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HAVE NOT HAD A CHANCE TO USE IT YET BUT IM SUR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>perfect, best iv'e had yet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The  coffee grinder is very lightweight, extre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This is a great cheese cutter. Smooth....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Great gift idea for alcoholic women. Xmas is c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is what it is... A awesome spill proof coff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Just what i needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>We have used several times - Love it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Good items</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>After less than nine months the thermal decant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I love these; I've bought expensive versions a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       star_rating  class                                        review_body\n",
       "0                1    2.0  Really didn't work as well as the pictures. Wh...\n",
       "1                1    2.0  brand new out of the box and it wouldn't even ...\n",
       "2                1    2.0  pure junk, lowest quality you could possibly g...\n",
       "3                1    2.0  Really bummed! Machine worked great until a fe...\n",
       "4                1    2.0  Sure, it cuts things, but the blades don't hav...\n",
       "5                1    2.0  I bought a couple bambu bowls 3 months ago, an...\n",
       "6                1    2.0  I bought this for my son. It arrived cracked, ...\n",
       "7                1    2.0  was impossible to use because did not fit any ...\n",
       "8                1    2.0                               Flavors aren't good.\n",
       "9                1    2.0  The bristles are way too soft to move any silk...\n",
       "10               1    2.0  After two months (and just after the return wi...\n",
       "11               1    2.0  Was shipped fast and we received it on time. N...\n",
       "12               1    2.0  I absolutely would not buy or recommend this p...\n",
       "13               1    2.0  This is the most poorly made item I have ever ...\n",
       "14               1    2.0  I bought the cookware set through Groupon and ...\n",
       "15               1    2.0  OK - I loved this machine at first but by opin...\n",
       "16               1    2.0  Like others who have the identical experience,...\n",
       "17               1    2.0  I received the package and when I opened it, i...\n",
       "18               1    2.0  While the picture and the description were a p...\n",
       "19               1    2.0  We ordered two sets of these glasses and they ...\n",
       "20               1    2.0  Never got it to work. Had to return it. The Ke...\n",
       "21               1    2.0  The scale did not work when I received it.  In...\n",
       "22               1    2.0  Wanted to save a couple of dollars and bought ...\n",
       "23               1    2.0  I bought this slow cooker after reading the Co...\n",
       "24               1    2.0            I returned it. It was very poorly made.\n",
       "25               1    2.0  Makes very, very weak coffee when used, even w...\n",
       "26               1    2.0  Pure crap. Poorly made. Not worth the price by...\n",
       "27               1    2.0  Warning is on the packaging but nowhere on Ama...\n",
       "28               1    2.0             Barely gets warm. Hub part works fine.\n",
       "29               1    2.0  It did great for a few weeks. The straw fell o...\n",
       "...            ...    ...                                                ...\n",
       "19970            5    1.0  LOVE this gel paste!  I will be buying this fo...\n",
       "19971            5    1.0                       Very happy with my purchase!\n",
       "19972            5    1.0  Makes an excellent, hot cup of coffee! Nice to...\n",
       "19973            5    1.0  My husband had a curved paring knife in his ki...\n",
       "19974            5    1.0                           Beautiful and practical!\n",
       "19975            5    1.0  exactly what I needed to make fresh squeezed j...\n",
       "19976            5    1.0  Brews coffee in minutes. Good size for two people\n",
       "19977            5    1.0     Love Mine so bought it for a friend as a gift.\n",
       "19978            5    1.0  OXO makes two different versions of this...ide...\n",
       "19979            5    1.0               looks great and like that its a 16oz\n",
       "19980            5    1.0  I was a skeptic at first but this is one of th...\n",
       "19981            5    1.0  This is a nice set of two salt or pepper mills...\n",
       "19982            5    1.0  Ease of use, speed of cooking, (be sure to pre...\n",
       "19983            5    1.0  I've used this pan for years and could not fin...\n",
       "19984            5    1.0  Used this once, and it worked much better than...\n",
       "19985            5    1.0  Theses favors are great quality and packaged v...\n",
       "19986            5    1.0  I've been making my pies in my old pyrex dish....\n",
       "19987            5    1.0  I love these colorful mugs...it is exactly as ...\n",
       "19988            5    1.0  Good container<br />Perfect Size<br />Ordered ...\n",
       "19989            5    1.0  HAVE NOT HAD A CHANCE TO USE IT YET BUT IM SUR...\n",
       "19990            5    1.0                        perfect, best iv'e had yet.\n",
       "19991            5    1.0  The  coffee grinder is very lightweight, extre...\n",
       "19992            5    1.0          This is a great cheese cutter. Smooth....\n",
       "19993            5    1.0  Great gift idea for alcoholic women. Xmas is c...\n",
       "19994            5    1.0  It is what it is... A awesome spill proof coff...\n",
       "19995            5    1.0                                 Just what i needed\n",
       "19996            5    1.0              We have used several times - Love it!\n",
       "19997            5    1.0                                         Good items\n",
       "19998            5    1.0  After less than nine months the thermal decant...\n",
       "19999            5    1.0  I love these; I've bought expensive versions a...\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To save memory, only keep three columns ['star_rating','class','review_body']\n",
    "review = new2[['star_rating','class','review_body']]\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Use data cleaning and preprocessing in order to include only important words from each review and improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(review):\n",
    "    # convert the all reviews into the lower case\n",
    "    review[\"preprocess_review\"] = review[\"review_body\"].str.lower()\n",
    "    # remove HTML from the reviews\n",
    "    review[\"preprocess_review\"] = review[\"preprocess_review\"].apply(lambda x: BeautifulSoup(x).get_text())# remove URLs from the reviews\n",
    "   # remove URLs from the reviews\n",
    "    review[\"preprocess_review\"] = review[\"preprocess_review\"].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')\n",
    "    # remove non-alphabetical characters\n",
    "    review[\"preprocess_review\"] = review[\"preprocess_review\"].str.replace('[^a-zA-Z\\'+\\'+]', ' ')\n",
    "    # remove the extra spaces between the words\n",
    "    review[\"preprocess_review\"] = review[\"preprocess_review\"].replace('\\s+', ' ', regex=True)\n",
    "    # perform contractions on the reviews\n",
    "    review[\"preprocess_review\"] = review[\"preprocess_review\"].apply(lambda x: contractions.fix(x))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gmlwl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = (set(stopwords.words(\"english\")) )\n",
    "\n",
    "# remore stop_words\n",
    "def removeStop(s): \n",
    "    s_list = s.split()\n",
    "    final_list = [word for word in s_list if word not in stop_words]\n",
    "    final_string = ' '.join(final_list)\n",
    "    return final_string\n",
    "\n",
    "# Perform lemmatizer\n",
    "def lemmatization_function(s):\n",
    "    s_list = s.split()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    final_list = [wordnet_lemmatizer.lemmatize(x) for x in s_list]\n",
    "    final_string = ' '.join(final_list)\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(review):\n",
    "    # remove the stop words\n",
    "    review[\"preprocess_review\"] = review[\"preprocess_review\"].apply(lambda x: removeStop(x))\n",
    "    \n",
    "    # perform lemmatization\n",
    "    review[\"preprocess_review\"] = review[\"preprocess_review\"].apply(lambda x: lemmatization_function(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement data cleaning and preprocessing\n",
    "data_cleaning(review)\n",
    "data_preprocessing(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>class</th>\n",
       "      <th>review_body</th>\n",
       "      <th>preprocess_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Really didn't work as well as the pictures. Wh...</td>\n",
       "      <td>really work well picture mug awake still see c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>brand new out of the box and it wouldn't even ...</td>\n",
       "      <td>brand new box would even dig cork let alone pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>pure junk lowest quality could possibly get gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Really bummed! Machine worked great until a fe...</td>\n",
       "      <td>really bummed machine worked great month ago t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Sure, it cuts things, but the blades don't hav...</td>\n",
       "      <td>sure cut thing blade enough force behind cut d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I bought a couple bambu bowls 3 months ago, an...</td>\n",
       "      <td>bought couple bambu bowl month ago bamboo laye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I bought this for my son. It arrived cracked, ...</td>\n",
       "      <td>bought son arrived cracked returned replacement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>was impossible to use because did not fit any ...</td>\n",
       "      <td>impossible use fit pot wound donating goodwill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Flavors aren't good.</td>\n",
       "      <td>flavor good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The bristles are way too soft to move any silk...</td>\n",
       "      <td>bristle way soft move silk save money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>After two months (and just after the return wi...</td>\n",
       "      <td>two month return window closed kettle suddenly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Was shipped fast and we received it on time. N...</td>\n",
       "      <td>shipped fast received time good buy zipper bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I absolutely would not buy or recommend this p...</td>\n",
       "      <td>absolutely would buy recommend producst two ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>This is the most poorly made item I have ever ...</td>\n",
       "      <td>poorly made item ever bought online even thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I bought the cookware set through Groupon and ...</td>\n",
       "      <td>bought cookware set groupon dissapointed poor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>OK - I loved this machine at first but by opin...</td>\n",
       "      <td>ok loved machine first opinion changed purchas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Like others who have the identical experience,...</td>\n",
       "      <td>like others identical experience unit simply f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I received the package and when I opened it, i...</td>\n",
       "      <td>received package opened noticed one eight squa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>While the picture and the description were a p...</td>\n",
       "      <td>picture description perfect match replacement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>We ordered two sets of these glasses and they ...</td>\n",
       "      <td>ordered two set glass arrived today reading de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Never got it to work. Had to return it. The Ke...</td>\n",
       "      <td>never got work return keurig cuisinart s terri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>The scale did not work when I received it.  In...</td>\n",
       "      <td>scale work received installed battery got err ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Wanted to save a couple of dollars and bought ...</td>\n",
       "      <td>wanted save couple dollar bought used kettle d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I bought this slow cooker after reading the Co...</td>\n",
       "      <td>bought slow cooker reading cook illustrated gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I returned it. It was very poorly made.</td>\n",
       "      <td>returned poorly made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Makes very, very weak coffee when used, even w...</td>\n",
       "      <td>make weak coffee used even expresso much plast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Pure crap. Poorly made. Not worth the price by...</td>\n",
       "      <td>pure crap poorly made worth price stretch imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Warning is on the packaging but nowhere on Ama...</td>\n",
       "      <td>warning packaging nowhere amazon's product des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Barely gets warm. Hub part works fine.</td>\n",
       "      <td>barely get warm hub part work fine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>It did great for a few weeks. The straw fell o...</td>\n",
       "      <td>great week straw fell lot put far would work f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LOVE this gel paste!  I will be buying this fo...</td>\n",
       "      <td>love gel paste buying formula different color ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Very happy with my purchase!</td>\n",
       "      <td>happy purchase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Makes an excellent, hot cup of coffee! Nice to...</td>\n",
       "      <td>make excellent hot cup coffee nice rid carafe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My husband had a curved paring knife in his ki...</td>\n",
       "      <td>husband curved paring knife kitchen met I neve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Beautiful and practical!</td>\n",
       "      <td>beautiful practical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>exactly what I needed to make fresh squeezed j...</td>\n",
       "      <td>exactly needed make fresh squeezed juice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Brews coffee in minutes. Good size for two people</td>\n",
       "      <td>brew coffee minute good size two people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Love Mine so bought it for a friend as a gift.</td>\n",
       "      <td>love mine bought friend gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>OXO makes two different versions of this...ide...</td>\n",
       "      <td>oxo make two different version identical lid a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>looks great and like that its a 16oz</td>\n",
       "      <td>look great like oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I was a skeptic at first but this is one of th...</td>\n",
       "      <td>skeptic first one product make happy purchase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This is a nice set of two salt or pepper mills...</td>\n",
       "      <td>nice set two salt pepper mill enjoy fresh crac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ease of use, speed of cooking, (be sure to pre...</td>\n",
       "      <td>ease use speed cooking sure preheat full minut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I've used this pan for years and could not fin...</td>\n",
       "      <td>I used pan year could find local retail store ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Used this once, and it worked much better than...</td>\n",
       "      <td>used worked much better previous silicone mat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Theses favors are great quality and packaged v...</td>\n",
       "      <td>thesis favor great quality packaged neatly wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I've been making my pies in my old pyrex dish....</td>\n",
       "      <td>I making pie old pyrex dish cleanup never easy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I love these colorful mugs...it is exactly as ...</td>\n",
       "      <td>love colorful mug exactly saw online would rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Good container&lt;br /&gt;Perfect Size&lt;br /&gt;Ordered ...</td>\n",
       "      <td>good containerperfect sizeordered two moreperf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HAVE NOT HAD A CHANCE TO USE IT YET BUT IM SUR...</td>\n",
       "      <td>chance use yet I sure awesome sorry cannot com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>perfect, best iv'e had yet.</td>\n",
       "      <td>perfect best iv'e yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The  coffee grinder is very lightweight, extre...</td>\n",
       "      <td>coffee grinder lightweight extremely easy use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>This is a great cheese cutter. Smooth....</td>\n",
       "      <td>great cheese cutter smooth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Great gift idea for alcoholic women. Xmas is c...</td>\n",
       "      <td>great gift idea alcoholic woman xmas coming gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It is what it is... A awesome spill proof coff...</td>\n",
       "      <td>awesome spill proof coffee mug first one dent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Just what i needed</td>\n",
       "      <td>needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>We have used several times - Love it!</td>\n",
       "      <td>used several time love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Good items</td>\n",
       "      <td>good item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>After less than nine months the thermal decant...</td>\n",
       "      <td>le nine month thermal decanter get hot touch n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I love these; I've bought expensive versions a...</td>\n",
       "      <td>love I bought expensive version funky angle st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       star_rating  class                                        review_body  \\\n",
       "0                1    2.0  Really didn't work as well as the pictures. Wh...   \n",
       "1                1    2.0  brand new out of the box and it wouldn't even ...   \n",
       "2                1    2.0  pure junk, lowest quality you could possibly g...   \n",
       "3                1    2.0  Really bummed! Machine worked great until a fe...   \n",
       "4                1    2.0  Sure, it cuts things, but the blades don't hav...   \n",
       "5                1    2.0  I bought a couple bambu bowls 3 months ago, an...   \n",
       "6                1    2.0  I bought this for my son. It arrived cracked, ...   \n",
       "7                1    2.0  was impossible to use because did not fit any ...   \n",
       "8                1    2.0                               Flavors aren't good.   \n",
       "9                1    2.0  The bristles are way too soft to move any silk...   \n",
       "10               1    2.0  After two months (and just after the return wi...   \n",
       "11               1    2.0  Was shipped fast and we received it on time. N...   \n",
       "12               1    2.0  I absolutely would not buy or recommend this p...   \n",
       "13               1    2.0  This is the most poorly made item I have ever ...   \n",
       "14               1    2.0  I bought the cookware set through Groupon and ...   \n",
       "15               1    2.0  OK - I loved this machine at first but by opin...   \n",
       "16               1    2.0  Like others who have the identical experience,...   \n",
       "17               1    2.0  I received the package and when I opened it, i...   \n",
       "18               1    2.0  While the picture and the description were a p...   \n",
       "19               1    2.0  We ordered two sets of these glasses and they ...   \n",
       "20               1    2.0  Never got it to work. Had to return it. The Ke...   \n",
       "21               1    2.0  The scale did not work when I received it.  In...   \n",
       "22               1    2.0  Wanted to save a couple of dollars and bought ...   \n",
       "23               1    2.0  I bought this slow cooker after reading the Co...   \n",
       "24               1    2.0            I returned it. It was very poorly made.   \n",
       "25               1    2.0  Makes very, very weak coffee when used, even w...   \n",
       "26               1    2.0  Pure crap. Poorly made. Not worth the price by...   \n",
       "27               1    2.0  Warning is on the packaging but nowhere on Ama...   \n",
       "28               1    2.0             Barely gets warm. Hub part works fine.   \n",
       "29               1    2.0  It did great for a few weeks. The straw fell o...   \n",
       "...            ...    ...                                                ...   \n",
       "19970            5    1.0  LOVE this gel paste!  I will be buying this fo...   \n",
       "19971            5    1.0                       Very happy with my purchase!   \n",
       "19972            5    1.0  Makes an excellent, hot cup of coffee! Nice to...   \n",
       "19973            5    1.0  My husband had a curved paring knife in his ki...   \n",
       "19974            5    1.0                           Beautiful and practical!   \n",
       "19975            5    1.0  exactly what I needed to make fresh squeezed j...   \n",
       "19976            5    1.0  Brews coffee in minutes. Good size for two people   \n",
       "19977            5    1.0     Love Mine so bought it for a friend as a gift.   \n",
       "19978            5    1.0  OXO makes two different versions of this...ide...   \n",
       "19979            5    1.0               looks great and like that its a 16oz   \n",
       "19980            5    1.0  I was a skeptic at first but this is one of th...   \n",
       "19981            5    1.0  This is a nice set of two salt or pepper mills...   \n",
       "19982            5    1.0  Ease of use, speed of cooking, (be sure to pre...   \n",
       "19983            5    1.0  I've used this pan for years and could not fin...   \n",
       "19984            5    1.0  Used this once, and it worked much better than...   \n",
       "19985            5    1.0  Theses favors are great quality and packaged v...   \n",
       "19986            5    1.0  I've been making my pies in my old pyrex dish....   \n",
       "19987            5    1.0  I love these colorful mugs...it is exactly as ...   \n",
       "19988            5    1.0  Good container<br />Perfect Size<br />Ordered ...   \n",
       "19989            5    1.0  HAVE NOT HAD A CHANCE TO USE IT YET BUT IM SUR...   \n",
       "19990            5    1.0                        perfect, best iv'e had yet.   \n",
       "19991            5    1.0  The  coffee grinder is very lightweight, extre...   \n",
       "19992            5    1.0          This is a great cheese cutter. Smooth....   \n",
       "19993            5    1.0  Great gift idea for alcoholic women. Xmas is c...   \n",
       "19994            5    1.0  It is what it is... A awesome spill proof coff...   \n",
       "19995            5    1.0                                 Just what i needed   \n",
       "19996            5    1.0              We have used several times - Love it!   \n",
       "19997            5    1.0                                         Good items   \n",
       "19998            5    1.0  After less than nine months the thermal decant...   \n",
       "19999            5    1.0  I love these; I've bought expensive versions a...   \n",
       "\n",
       "                                       preprocess_review  \n",
       "0      really work well picture mug awake still see c...  \n",
       "1      brand new box would even dig cork let alone pu...  \n",
       "2      pure junk lowest quality could possibly get gi...  \n",
       "3      really bummed machine worked great month ago t...  \n",
       "4      sure cut thing blade enough force behind cut d...  \n",
       "5      bought couple bambu bowl month ago bamboo laye...  \n",
       "6        bought son arrived cracked returned replacement  \n",
       "7         impossible use fit pot wound donating goodwill  \n",
       "8                                            flavor good  \n",
       "9                  bristle way soft move silk save money  \n",
       "10     two month return window closed kettle suddenly...  \n",
       "11     shipped fast received time good buy zipper bro...  \n",
       "12     absolutely would buy recommend producst two ye...  \n",
       "13     poorly made item ever bought online even thoug...  \n",
       "14     bought cookware set groupon dissapointed poor ...  \n",
       "15     ok loved machine first opinion changed purchas...  \n",
       "16     like others identical experience unit simply f...  \n",
       "17     received package opened noticed one eight squa...  \n",
       "18     picture description perfect match replacement ...  \n",
       "19     ordered two set glass arrived today reading de...  \n",
       "20     never got work return keurig cuisinart s terri...  \n",
       "21     scale work received installed battery got err ...  \n",
       "22     wanted save couple dollar bought used kettle d...  \n",
       "23     bought slow cooker reading cook illustrated gl...  \n",
       "24                                  returned poorly made  \n",
       "25     make weak coffee used even expresso much plast...  \n",
       "26     pure crap poorly made worth price stretch imag...  \n",
       "27     warning packaging nowhere amazon's product des...  \n",
       "28                    barely get warm hub part work fine  \n",
       "29     great week straw fell lot put far would work f...  \n",
       "...                                                  ...  \n",
       "19970  love gel paste buying formula different color ...  \n",
       "19971                                     happy purchase  \n",
       "19972  make excellent hot cup coffee nice rid carafe ...  \n",
       "19973  husband curved paring knife kitchen met I neve...  \n",
       "19974                                beautiful practical  \n",
       "19975           exactly needed make fresh squeezed juice  \n",
       "19976            brew coffee minute good size two people  \n",
       "19977                       love mine bought friend gift  \n",
       "19978  oxo make two different version identical lid a...  \n",
       "19979                                 look great like oz  \n",
       "19980  skeptic first one product make happy purchase ...  \n",
       "19981  nice set two salt pepper mill enjoy fresh crac...  \n",
       "19982  ease use speed cooking sure preheat full minut...  \n",
       "19983  I used pan year could find local retail store ...  \n",
       "19984  used worked much better previous silicone mat ...  \n",
       "19985  thesis favor great quality packaged neatly wou...  \n",
       "19986  I making pie old pyrex dish cleanup never easy...  \n",
       "19987  love colorful mug exactly saw online would rec...  \n",
       "19988  good containerperfect sizeordered two moreperf...  \n",
       "19989  chance use yet I sure awesome sorry cannot com...  \n",
       "19990                              perfect best iv'e yet  \n",
       "19991  coffee grinder lightweight extremely easy use ...  \n",
       "19992                         great cheese cutter smooth  \n",
       "19993  great gift idea alcoholic woman xmas coming gr...  \n",
       "19994  awesome spill proof coffee mug first one dent ...  \n",
       "19995                                             needed  \n",
       "19996                             used several time love  \n",
       "19997                                          good item  \n",
       "19998  le nine month thermal decanter get hot touch n...  \n",
       "19999  love I bought expensive version funky angle st...  \n",
       "\n",
       "[20000 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train data and test data (80%:20%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(review['preprocess_review'], review['class'], test_size=0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### (2) Make the Word2Vec features as a input using my own Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sg = 1 : skip-gram\n",
    "wm_model = Word2Vec(sentences=tokenize(review['preprocess_review']), vector_size=300, window=11, min_count=10, workers=4, sg=1)\n",
    "wm_model.save(\"new_word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_vector(X, Y):\n",
    "    \n",
    "    total_vector = tokenize(X)\n",
    "    new_X = []\n",
    "    remove_index = []\n",
    "    for idx,sentence in zip(X.index,total_vector):\n",
    "        average = [0,]\n",
    "        words = list(filter(lambda x: x in wm_model.wv.index_to_key, sentence)) # Filtering. only keep existed words\n",
    "        if len(words) == 0 : # If list 'words' is empty, we have to remove it, So keep the index value.\n",
    "            remove_index.append(idx)\n",
    "            continue\n",
    "        else:\n",
    "            for word in words:\n",
    "                average += wm_model.wv[word]\n",
    "            new_X.append(average / len(words))\n",
    "\n",
    "    # Remove the Y_train value paired with the removed X_train\n",
    "    new_Y = Y.drop(labels=remove_index)\n",
    "            \n",
    "    return new_X, new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm_X_train, wm_Y_train = change_to_vector(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm_X_test, wm_Y_test = change_to_vector(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Train and test perceptron with my own Word2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Perceptron and test data\n",
    "pct = Perceptron(tol=1e-3, random_state=2)\n",
    "pct.fit(wm_X_train, wm_Y_train)\n",
    "pct_y_test_pred = pct.predict(wm_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My own Word2Vec] Perceptron_Test_Accuracy: 0.750125\n",
      "[My own Word2Vec] Perceptron_Test_Precision: 0.685045\n",
      "[My own Word2Vec] Perceptron_Test_Recall: 0.938765\n",
      "[My own Word2Vec] Perceptron_Test_F1 Score: 0.792083\n"
     ]
    }
   ],
   "source": [
    "print('[My own Word2Vec] Perceptron_Test_Accuracy: %f' % accuracy_score(wm_Y_test, pct_y_test_pred))\n",
    "print('[My own Word2Vec] Perceptron_Test_Precision: %f' % precision_score(wm_Y_test, pct_y_test_pred))\n",
    "print('[My own Word2Vec] Perceptron_Test_Recall: %f' % recall_score(wm_Y_test, pct_y_test_pred))\n",
    "print('[My own Word2Vec] Perceptron_Test_F1 Score: %f' % f1_score(wm_Y_test, pct_y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Train and Test SVM with my own Word2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM and test data\n",
    "svm = LinearSVC(random_state=2)\n",
    "svm.fit(wm_X_train, wm_Y_train)\n",
    "svm_y_test_pred = svm.predict(wm_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My own Word2Vec] SVM_Test_Accuracy: 0.817226\n",
      "[My own Word2Vec] SVM_Test_Precision: 0.829517\n",
      "[My own Word2Vec] SVM_Test_Recall: 0.804938\n",
      "[My own Word2Vec] SVM_Test_F1 Score: 0.817043\n"
     ]
    }
   ],
   "source": [
    "print('[My own Word2Vec] SVM_Test_Accuracy: %f' % accuracy_score(wm_Y_test, svm_y_test_pred))\n",
    "print('[My own Word2Vec] SVM_Test_Precision: %f' % precision_score(wm_Y_test, svm_y_test_pred))\n",
    "print('[My own Word2Vec] SVM_Test_Recall: %f' % recall_score(wm_Y_test, svm_y_test_pred))\n",
    "print('[My own Word2Vec] SVM_Test_F1 Score: %f' % f1_score(wm_Y_test, svm_y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) Make the Word2Vec features as a input using \"word2vec-google-news-300.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_wv_google_news(X, Y):\n",
    "    \n",
    "    total_vector = tokenize(X)\n",
    "    new_X = []\n",
    "    remove_index = []\n",
    "    for idx,sentence in zip(X.index,total_vector):\n",
    "        average = [0,]\n",
    "        words = list(filter(lambda x: x in wv.index_to_key, sentence)) # Filtering. Only keep existed words\n",
    "        if len(words) == 0 : # If list 'words' is empty, we have to remove it, So keep the index value.\n",
    "            remove_index.append(idx)\n",
    "            continue\n",
    "        else:\n",
    "            for word in words:\n",
    "                average += wv[word]\n",
    "            new_X.append(average / len(words))\n",
    "\n",
    "    # Remove the Y_train value paired with the removed X_train\n",
    "    new_Y = Y.drop(labels=remove_index)\n",
    "            \n",
    "    return new_X, new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_X_train, gn_Y_train = change_to_wv_google_news(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_X_test, gn_Y_test = change_to_wv_google_news(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) Train and test perceptron with \"word2vec-google-news-300\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Perceptron and test data\n",
    "pct = Perceptron(tol=1e-3, random_state=2)\n",
    "pct.fit(gn_X_train, gn_Y_train)\n",
    "gn_pct_y_test_pred = pct.predict(gn_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[google-news-300] Perceptron_Test_Accuracy: 0.776527\n",
      "[google-news-300] Perceptron_Test_Precision: 0.852174\n",
      "[google-news-300] Perceptron_Test_Recall: 0.676862\n",
      "[google-news-300] Perceptron_Test_F1 Score: 0.754468\n"
     ]
    }
   ],
   "source": [
    "print('[google-news-300] Perceptron_Test_Accuracy: %f' % accuracy_score(gn_Y_test, gn_pct_y_test_pred))\n",
    "print('[google-news-300] Perceptron_Test_Precision: %f' % precision_score(gn_Y_test, gn_pct_y_test_pred))\n",
    "print('[google-news-300] Perceptron_Test_Recall: %f' % recall_score(gn_Y_test, gn_pct_y_test_pred))\n",
    "print('[google-news-300] Perceptron_Test_F1 Score: %f' % f1_score(gn_Y_test, gn_pct_y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) Train and Test SVM with \"word2vec-google-news-300\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM and test data\n",
    "svm = LinearSVC(random_state=2)\n",
    "svm.fit(gn_X_train, gn_Y_train)\n",
    "gn_svm_y_test_pred = svm.predict(gn_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[google-news-300] SVM_Test_Accuracy: 0.807808\n",
      "[google-news-300] SVM_Test_Precision: 0.824987\n",
      "[google-news-300] SVM_Test_Recall: 0.788357\n",
      "[google-news-300] SVM_Test_F1 Score: 0.806256\n"
     ]
    }
   ],
   "source": [
    "print('[google-news-300] SVM_Test_Accuracy: %f' % accuracy_score(gn_Y_test, gn_svm_y_test_pred))\n",
    "print('[google-news-300] SVM_Test_Precision: %f' % precision_score(gn_Y_test, gn_svm_y_test_pred))\n",
    "print('[google-news-300] SVM_Test_Recall: %f' % recall_score(gn_Y_test, gn_svm_y_test_pred))\n",
    "print('[google-news-300] SVM_Test_F1 Score: %f' % f1_score(gn_Y_test, gn_svm_y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) Create Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvector = TfidfVectorizer()\n",
    "tf_x_train = tfvector.fit_transform(X_train)\n",
    "tf_x_test = tfvector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 16000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (9) Train and test perceptron with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = Perceptron(tol=1e-3, random_state=2)\n",
    "pct.fit(tf_x_train, Y_train)\n",
    "pct_y_train_pred = pct.predict(tf_x_train)\n",
    "tf_pct_y_test_pred = pct.predict(tf_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF-IDF] Perceptron_Test_Accuracy: 0.800000\n",
      "[TF-IDF] Perceptron_Test_Precision: 0.804757\n",
      "[TF-IDF] Perceptron_Test_Recall: 0.800000\n",
      "[TF-IDF] Perceptron_Test_F1 Score: 0.802372\n"
     ]
    }
   ],
   "source": [
    "print('[TF-IDF] Perceptron_Test_Accuracy: %f' % accuracy_score(Y_test, tf_pct_y_test_pred))\n",
    "print('[TF-IDF] Perceptron_Test_Precision: %f' % precision_score(Y_test, tf_pct_y_test_pred))\n",
    "print('[TF-IDF] Perceptron_Test_Recall: %f' % recall_score(Y_test, tf_pct_y_test_pred))\n",
    "print('[TF-IDF] Perceptron_Test_F1 Score: %f' % f1_score(Y_test, tf_pct_y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (10) Train and test SVM with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(random_state=2)\n",
    "svm.fit(tf_x_train, Y_train)\n",
    "svm_y_train_pred = svm.predict(tf_x_train)\n",
    "tf_svm_y_test_pred = svm.predict(tf_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF-IDF] SVM_Test_Accuracy: 0.846000\n",
      "[TF-IDF] SVM_Test_Precision: 0.855634\n",
      "[TF-IDF] SVM_Test_Recall: 0.837931\n",
      "[TF-IDF] SVM_Test_F1 Score: 0.846690\n"
     ]
    }
   ],
   "source": [
    "print('[TF-IDF] SVM_Test_Accuracy: %f' % accuracy_score(Y_test, tf_svm_y_test_pred))\n",
    "print('[TF-IDF] SVM_Test_Precision: %f' % precision_score(Y_test, tf_svm_y_test_pred))\n",
    "print('[TF-IDF] SVM_Test_Recall: %f' % recall_score(Y_test, tf_svm_y_test_pred))\n",
    "print('[TF-IDF] SVM_Test_F1 Score: %f' % f1_score(Y_test, tf_svm_y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report \n",
    "#### \"word2Vec-google-news-300\" VS \"my own Word2Vec model\" VS \"TF-IDF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='3-perceptron_svm.PNG'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Among the three feature types, the model using TF-IDF as a feature showed the best performance. Comparing my own word2vec and word2vec-google-news, the performance difference between the two is very small, so the performance effect will vary depending on what kind of data the training data is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Feedforward Neural Networks\n",
    "\n",
    "reference : https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Using the average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- binary classification using class 1 and class 2\n",
    "- ternary model for the three class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Make binary classification datasets (class 1 = rating 4, 5) (class 2 = rating 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a binary classification dataset\n",
    "new = pd.read_csv('binary_data.csv', index_col=0)\n",
    "bi = new.loc[new['class'] < 3].reset_index(drop=True)\n",
    "bi = bi[['star_rating','class','review_body']]\n",
    "\n",
    "# implement data cleaning and preprocessing\n",
    "data_cleaning(bi)\n",
    "data_preprocessing(bi)\n",
    "\n",
    "# For split train data and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# using my own Word2Vec model\n",
    "temp_X, temp_Y = change_to_vector(bi['preprocess_review'], bi['class'])\n",
    "bi_mw_X_train, bi_mw_X_test, bi_mw_Y_train, bi_mw_Y_test = train_test_split(temp_X, temp_Y, test_size=0.2, random_state = 2)\n",
    "\n",
    "# using \"word2vec-google-news-300\" modoel\n",
    "temp_X2, temp_Y2 = change_to_wv_google_news(bi['preprocess_review'], bi['class'])\n",
    "bi_gn_X_train, bi_gn_X_test, bi_gn_Y_train, bi_gn_Y_test = train_test_split(temp_X2, temp_Y2, test_size=0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Make ternary classification datasets (class 1 = rating 4, 5) (class 2 = rating 1, 2) (class 3 = rating 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a ternary model for three class\n",
    "new = pd.read_csv('ternary_data.csv', index_col=0)\n",
    "\n",
    "# To create ternary labels, mapping the ratings.\n",
    "new.loc[(new['star_rating'] > 3), 'class'] = 1\n",
    "new.loc[(new['star_rating'] < 3), 'class'] = 2\n",
    "new.loc[(new['star_rating'] == 3), 'class'] = 3\n",
    "\n",
    "ten = new[['star_rating','class','review_body']]\n",
    "\n",
    "# implement data cleaning and preprocessing\n",
    "data_cleaning(ten)\n",
    "data_preprocessing(ten)\n",
    "\n",
    "# split train data and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# using my own Word2Vec model\n",
    "temp_X3, temp_Y3 = change_to_vector(ten['preprocess_review'], ten['class'])\n",
    "ten_wm_X_train, ten_wm_X_test, ten_wm_Y_train, ten_wm_Y_test = train_test_split(temp_X3, temp_Y3, test_size=0.2, random_state = 2)\n",
    "\n",
    "# using \"word2vec-google-news-300\" modoel\n",
    "temp_X4, temp_Y4 = change_to_wv_google_news(ten['preprocess_review'], ten['class'])\n",
    "ten_gn_X_train, ten_gn_X_test, ten_gn_Y_train, ten_gn_Y_test = train_test_split(temp_X4, temp_Y4, test_size=0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Make a Feedforward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Feedforward model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(300, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()  # Model train\n",
    "\n",
    "    # Train model with mini batch\n",
    "    for data, targets in loader_train:\n",
    "\n",
    "        optimizer.zero_grad()  # Initiate\n",
    "        outputs = model(data)  # model train and output\n",
    "        loss = loss_fn(outputs, targets)  # Calculate loss values (real value - predicted value)\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Edit weights\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, 100, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()  # Test Model\n",
    "    correct = 0\n",
    "\n",
    "    # Create minibatch\n",
    "    with torch.no_grad():\n",
    "        for data, targets in loader_test:\n",
    "\n",
    "            outputs = model(data)  # Put input data and get output data\n",
    "\n",
    "            # Calculate correct case\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Calculate which label has the highest probability\n",
    "            correct += predicted.eq(targets.data.view_as(predicted)).sum()  # If it matches the answer, increase the count\n",
    "\n",
    "    # Print accuracy\n",
    "    data_num = len(loader_test.dataset)\n",
    "    print('\\nAccuracy with test data: {}/{} ({:.0f}%)\\n'.format(correct,\n",
    "                                                   data_num, 100. * correct / data_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "#### (5-1) Binary classification with average Word2Vec vectors that are made by my own Word2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data (chage target data. If the class is '2', change to '1') and change data type\n",
    "# Change datatype to tensor\n",
    "X_train = torch.Tensor(bi_mw_X_train)\n",
    "X_test = torch.Tensor(bi_mw_X_test)\n",
    "y_train = torch.LongTensor(bi_mw_Y_train-1)\n",
    "y_test = torch.LongTensor(bi_mw_Y_test.values-1)\n",
    "\n",
    "# Make a dataset and dataloader\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.578847\n",
      "Epoch   10/100 Cost: 0.349494\n",
      "Epoch   20/100 Cost: 0.488557\n",
      "Epoch   30/100 Cost: 0.304918\n",
      "Epoch   40/100 Cost: 0.306718\n",
      "Epoch   50/100 Cost: 0.303817\n",
      "Epoch   60/100 Cost: 0.171023\n",
      "Epoch   70/100 Cost: 0.174204\n",
      "Epoch   80/100 Cost: 0.297377\n",
      "Epoch   90/100 Cost: 0.203899\n",
      "\n",
      "Accuracy with test data: 3269/3994 (82%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train with epoch = 100, and test data\n",
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5-2) Binary classification with average Word2Vec vectors that are made by word2vec-google-news-300 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data (chage target data. If the class is '2', change to '1') and change data type\n",
    "# Change datatype to tensor\n",
    "X_train = torch.Tensor(bi_gn_X_train)\n",
    "X_test = torch.Tensor(bi_gn_X_test)\n",
    "y_train = torch.LongTensor(bi_gn_Y_train-1)\n",
    "y_test = torch.LongTensor(bi_gn_Y_test.values-1)\n",
    "\n",
    "# Make a dataset and dataloader\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.537454\n",
      "Epoch   10/100 Cost: 0.213030\n",
      "Epoch   20/100 Cost: 0.310074\n",
      "Epoch   30/100 Cost: 0.192528\n",
      "Epoch   40/100 Cost: 0.224727\n",
      "Epoch   50/100 Cost: 0.133237\n",
      "Epoch   60/100 Cost: 0.131064\n",
      "Epoch   70/100 Cost: 0.118380\n",
      "Epoch   80/100 Cost: 0.103861\n",
      "Epoch   90/100 Cost: 0.156226\n",
      "\n",
      "Accuracy with test data: 3122/3997 (78%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train with epoch = 100, and test data\n",
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tenery classification\n",
    "#### (6-1) Tenerary classification with average Word2Vec vectors that are made by my own Word2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(300, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data \n",
    "X_train = torch.Tensor(ten_wm_X_train)\n",
    "X_test = torch.Tensor(ten_wm_X_test)\n",
    "y_train = torch.LongTensor(ten_wm_Y_train-1) # Reduce each class number by 1\n",
    "y_test = torch.LongTensor(ten_wm_Y_test.values-1) # Reduce each class number by 1\n",
    "\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.901392\n",
      "Epoch   10/100 Cost: 0.692493\n",
      "Epoch   20/100 Cost: 0.836561\n",
      "Epoch   30/100 Cost: 0.581890\n",
      "Epoch   40/100 Cost: 0.621298\n",
      "Epoch   50/100 Cost: 0.622448\n",
      "Epoch   60/100 Cost: 0.671561\n",
      "Epoch   70/100 Cost: 0.554857\n",
      "Epoch   80/100 Cost: 0.586218\n",
      "Epoch   90/100 Cost: 0.582200\n",
      "\n",
      "Accuracy with test data: 3245/4992 (65%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train with epoch = 100, and test data\n",
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6-2) Ternary classification with average Word2Vec vectors that are made by word2vec-google-news-300 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data \n",
    "X_train = torch.Tensor(ten_gn_X_train)\n",
    "X_test = torch.Tensor(ten_gn_X_test)\n",
    "y_train = torch.LongTensor(ten_gn_Y_train-1) # Reduce each class number by 1\n",
    "y_test = torch.LongTensor(ten_gn_Y_test.values-1) # Reduce each class number by 1\n",
    "\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.829969\n",
      "Epoch   10/100 Cost: 0.794858\n",
      "Epoch   20/100 Cost: 0.601884\n",
      "Epoch   30/100 Cost: 0.560058\n",
      "Epoch   40/100 Cost: 0.367260\n",
      "Epoch   50/100 Cost: 0.484547\n",
      "Epoch   60/100 Cost: 0.845158\n",
      "Epoch   70/100 Cost: 0.536882\n",
      "Epoch   80/100 Cost: 0.200163\n",
      "Epoch   90/100 Cost: 0.314929\n",
      "\n",
      "Accuracy with test data: 3017/4996 (60%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train with epoch = 100, and test data\n",
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report - with the average Word2Vec vectors\n",
    "\n",
    "<img src = '4-a.PNG'>\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Concatenate the first 10 Word2Vec vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) To generate the input features, concatenate the first 10 Word2Vec vectors for each review as the input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate concatenate the first 10 word2vec vectors with my own Word2Vec model\n",
    "def concatenate_word2vec(X, Y):\n",
    "    total_vector = tokenize(X)\n",
    "    new_X = []\n",
    "    remove_index = []\n",
    "    for idx,sentence in zip(X.index,total_vector):\n",
    "        temp_X = []\n",
    "        words = list(filter(lambda x: x in wm_model.wv.index_to_key, sentence)) # Only keep existed words.\n",
    "        if len(words) == 0 : # If list 'words' is empty, we have to remove it, So keep the index value.\n",
    "            remove_index.append(idx)\n",
    "            continue\n",
    "        else: \n",
    "            for word in words[:10]: # first 10 Word3Vec vectors\n",
    "                temp_X = np.concatenate((temp_X, wm_model.wv[word]))\n",
    "        if len(temp_X) != 3000:\n",
    "            temp_X = np.pad(temp_X, (0,3000-len(temp_X)), 'constant', constant_values=0)\n",
    "        new_X.append(temp_X)\n",
    "\n",
    "    # Remove the Y_train value paired with the removed X_train\n",
    "    new_Y = Y.drop(labels=remove_index)\n",
    "    return new_X, new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate concatenate the first 10 word2vec vectors with Word2Vec-google-news model\n",
    "def concatenate_word2vec_google_news(X, Y):\n",
    "    total_vector = tokenize(X)\n",
    "    new_X = []\n",
    "    remove_index = []\n",
    "    for idx,sentence in zip(X.index,total_vector):\n",
    "        temp_X = []\n",
    "        words = list(filter(lambda x: x in wv.index_to_key, sentence)) # Only keep existed words.\n",
    "        if len(words) == 0 : # If list 'words' is empty, we have to remove it, So keep the index value.\n",
    "            remove_index.append(idx)\n",
    "            continue\n",
    "        else:\n",
    "            for word in words[:10]: # first 10 Word3Vec vectors\n",
    "                temp_X = np.concatenate((temp_X, wv[word]))\n",
    "        if len(temp_X) != 3000:\n",
    "            temp_X = np.pad(temp_X, (0,3000-len(temp_X)), 'constant', constant_values=0)\n",
    "        new_X.append(temp_X)\n",
    "\n",
    "    # Remove the Y_train value paired with the removed X_train\n",
    "    new_Y = Y.drop(labels=remove_index)\n",
    "            \n",
    "    return new_X, new_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Set data (concatenate the first 10 Word2Vec vectors for each review as the input feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using my own Word2Vec model\n",
    "temp_X5, temp_Y5 = concatenate_word2vec(bi['preprocess_review'], bi['class'])\n",
    "bi_mw10_X_train, bi_mw10_X_test, bi_mw10_Y_train, bi_mw10_Y_test = train_test_split(temp_X5, temp_Y5, test_size=0.2, random_state = 2)\n",
    "\n",
    "# using \"word2vec-google-news-300\" modoel\n",
    "temp_X6, temp_Y6 = concatenate_word2vec_google_news(bi['preprocess_review'], bi['class'])\n",
    "bi_gn10_X_train, bi_gn10_X_test, bi_gn10_Y_train, bi_gn10_Y_test = train_test_split(temp_X6, temp_Y6, test_size=0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using my own Word2Vec model\n",
    "temp_X7, temp_Y7 = concatenate_word2vec(ten['preprocess_review'], ten['class'])\n",
    "ten_wm10_X_train, ten_wm10_X_test, ten_wm10_Y_train, ten_wm10_Y_test = train_test_split(temp_X7, temp_Y7, test_size=0.2, random_state = 2)\n",
    "\n",
    "# using \"word2vec-google-news-300\" modoel\n",
    "temp_X8, temp_Y8 = concatenate_word2vec_google_news(ten['preprocess_review'], ten['class'])\n",
    "ten_gn10_X_train, ten_gn10_X_test, ten_gn10_Y_train, ten_gn10_Y_test = train_test_split(temp_X8, temp_Y8, test_size=0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "#### (3) Set model for binary classification with concatenate the first 10 Word2Vec voctors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3000, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4-1) With my own word2vec vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data (chage target data. If the class is '2', change to '1') and change data type\n",
    "# Change datatype to tensor\n",
    "X_train = torch.Tensor(bi_mw10_X_train)\n",
    "X_test = torch.Tensor(bi_mw10_X_test)\n",
    "y_train = torch.LongTensor(bi_mw10_Y_train-1) # Reduce each class number by 1\n",
    "y_test = torch.LongTensor(bi_mw10_Y_test.values-1) # Reduce each class number by 1\n",
    "\n",
    "# Make a dataset and dataloader\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.408697\n",
      "Epoch   10/100 Cost: 0.070430\n",
      "Epoch   20/100 Cost: 0.004547\n",
      "Epoch   30/100 Cost: 0.064171\n",
      "Epoch   40/100 Cost: 0.000299\n",
      "Epoch   50/100 Cost: 0.000494\n",
      "Epoch   60/100 Cost: 0.002284\n",
      "Epoch   70/100 Cost: 0.000079\n",
      "Epoch   80/100 Cost: 0.050645\n",
      "Epoch   90/100 Cost: 0.050819\n",
      "\n",
      "Accuracy with test data: 2908/3994 (73%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train with epoch = 100, and test data\n",
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4-2) With google news word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data (chage target data. If the class is '2', change to '1') and change data type\n",
    "# Change datatype to tensor\n",
    "X_train = torch.Tensor(bi_gn10_X_train)\n",
    "X_test = torch.Tensor(bi_gn10_X_test)\n",
    "y_train = torch.LongTensor(bi_gn10_Y_train-1) # Reduce each class number by 1\n",
    "y_test = torch.LongTensor(bi_gn10_Y_test.values-1) # Reduce each class number by 1\n",
    "\n",
    "# Make a dataset and dataloader\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.648548\n",
      "Epoch   10/100 Cost: 0.123390\n",
      "Epoch   20/100 Cost: 0.009467\n",
      "Epoch   30/100 Cost: 0.005616\n",
      "Epoch   40/100 Cost: 0.001202\n",
      "Epoch   50/100 Cost: 0.003593\n",
      "Epoch   60/100 Cost: 0.025437\n",
      "Epoch   70/100 Cost: 0.035908\n",
      "Epoch   80/100 Cost: 0.000258\n",
      "Epoch   90/100 Cost: 0.000211\n",
      "\n",
      "Accuracy with test data: 2883/3997 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train with epoch = 100, and test data\n",
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary classification\n",
    "#### (5) Set model for binary classification with concatenate the first 10 Word2Vec voctors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3000, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6-1) With my own word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data \n",
    "X_train = torch.Tensor(ten_wm10_X_train)\n",
    "X_test = torch.Tensor(ten_wm10_X_test)\n",
    "y_train = torch.LongTensor(ten_wm10_Y_train-1) # Reduce each class number by 1\n",
    "y_test = torch.LongTensor(ten_wm10_Y_test.values-1) # Reduce each class number by 1\n",
    "\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.873941\n",
      "Epoch   10/100 Cost: 0.465283\n",
      "Epoch   20/100 Cost: 0.099524\n",
      "Epoch   30/100 Cost: 0.142683\n",
      "Epoch   40/100 Cost: 0.060731\n",
      "Epoch   50/100 Cost: 0.011700\n",
      "Epoch   60/100 Cost: 0.015981\n",
      "Epoch   70/100 Cost: 0.013891\n",
      "Epoch   80/100 Cost: 0.005859\n",
      "Epoch   90/100 Cost: 0.030007\n",
      "\n",
      "Accuracy with test data: 2707/4992 (54%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train with epoch = 100, and test data\n",
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6-2) With google news word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data r\n",
    "X_train = torch.Tensor(ten_gn10_X_train)\n",
    "X_test = torch.Tensor(ten_gn10_X_test)\n",
    "y_train = torch.LongTensor(ten_gn10_Y_train-1) # Reduce each class number by 1\n",
    "y_test = torch.LongTensor(ten_gn10_Y_test.values-1) # Reduce each class number by 1\n",
    "\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.665067\n",
      "Epoch   10/100 Cost: 0.385254\n",
      "Epoch   20/100 Cost: 0.025130\n",
      "Epoch   30/100 Cost: 0.001866\n",
      "Epoch   40/100 Cost: 0.000823\n",
      "Epoch   50/100 Cost: 0.042108\n",
      "Epoch   60/100 Cost: 0.005129\n",
      "Epoch   70/100 Cost: 0.002946\n",
      "Epoch   80/100 Cost: 0.000190\n",
      "Epoch   90/100 Cost: 0.000655\n",
      "\n",
      "Accuracy with test data: 2611/4996 (52%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train with epoch = 100, and test data\n",
    "for epoch in range(100):\n",
    "    train(epoch)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report - with the concatenated the first 10 Word2Vec vectors\n",
    "\n",
    "<img src = '4-b.PNG'>\n",
    "[Report] => Looking at the results of Feedforward Neural Networks, overall, the model with the features that used my own word2vec vectors as an input has better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Conclude] - Total Report \n",
    "\n",
    "<img src = '4-c.PNG'>\n",
    "\n",
    "[Report] => The SVM model using TF-IDF features has the best performance compared to the other models. The Feedforward Neural Networks model using the average my own word2vec vectors has the second-best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5. Recurrent Neural Networks\n",
    "\n",
    "<br>\n",
    "<font color='red'> \n",
    "※[Important] Order :  <br></font> \n",
    "   1. (Using my word2vec model) Binary RNN <br>\n",
    "   2. (Using my word2vec model) Binary GRU <br>\n",
    "   3. (Using google-news-300 model) Binary RNN <br>\n",
    "   4. (Using google-news-300 model) Binary GRU <br>\n",
    "   5. (Using my word2vec model) Ternary RNN <br>\n",
    "   6. (Using my word2vec model) Ternary GRU <br>\n",
    "   7. (Using google-news-300 model) Ternary RNN <br>\n",
    "   8. (Using google-news-300 model) Ternary GRU<br>\n",
    "\n",
    "\n",
    "\n",
    "#### <font color='blue'> Unfortunately, I have computational resource limitations, especially memory issue, so I had to set epoch to 3. (epoch=3) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a-1-1) Binary model with RNN using my word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0) Using the features that I generated using the models I prepared in the \"Word Embedding\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>11903534</td>\n",
       "      <td>R3NBYY1PF9MXRT</td>\n",
       "      <td>B00FRMV38Y</td>\n",
       "      <td>872686823</td>\n",
       "      <td>Decodyne&amp;#0153; Morning Mug, Heat Sensitive Co...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Didn't deliver</td>\n",
       "      <td>Really didn't work as well as the pictures. Wh...</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>27885863</td>\n",
       "      <td>R1A10GP8CPG1A2</td>\n",
       "      <td>B003YFI0O6</td>\n",
       "      <td>111524501</td>\n",
       "      <td>Oster Electric Wine-Bottle Opener</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>didn't work even the first time</td>\n",
       "      <td>brand new out of the box and it wouldn't even ...</td>\n",
       "      <td>2014-03-21</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>15355157</td>\n",
       "      <td>R147NFWDLR0AT9</td>\n",
       "      <td>B0002T4ZL4</td>\n",
       "      <td>978772977</td>\n",
       "      <td>Oggi 5355 4-Piece Acrylic Canister Set with Ai...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>15647704</td>\n",
       "      <td>R1GQQLPV9LCY1T</td>\n",
       "      <td>B0034J6QIY</td>\n",
       "      <td>591197834</td>\n",
       "      <td>Cuisinart SS-700 Single Serve Brewing System -...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Cuisinart Keurig ruined my week!</td>\n",
       "      <td>Really bummed! Machine worked great until a fe...</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26424346</td>\n",
       "      <td>R1X5BB0UPZ4IWT</td>\n",
       "      <td>B000AXQA8I</td>\n",
       "      <td>330600737</td>\n",
       "      <td>Kuhn Rikon Twist and Chop, Artichoke</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Not what I was expecting</td>\n",
       "      <td>Sure, it cuts things, but the blades don't hav...</td>\n",
       "      <td>2007-02-20</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     11903534  R3NBYY1PF9MXRT  B00FRMV38Y       872686823   \n",
       "1          US     27885863  R1A10GP8CPG1A2  B003YFI0O6       111524501   \n",
       "2          US     15355157  R147NFWDLR0AT9  B0002T4ZL4       978772977   \n",
       "3          US     15647704  R1GQQLPV9LCY1T  B0034J6QIY       591197834   \n",
       "4          US     26424346  R1X5BB0UPZ4IWT  B000AXQA8I       330600737   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  Decodyne&#0153; Morning Mug, Heat Sensitive Co...          Kitchen   \n",
       "1                  Oster Electric Wine-Bottle Opener          Kitchen   \n",
       "2  Oggi 5355 4-Piece Acrylic Canister Set with Ai...          Kitchen   \n",
       "3  Cuisinart SS-700 Single Serve Brewing System -...          Kitchen   \n",
       "4               Kuhn Rikon Twist and Chop, Artichoke          Kitchen   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            1            0.0          0.0    N                 Y   \n",
       "1            1            1.0          1.0    N                 Y   \n",
       "2            1            2.0          2.0    N                 Y   \n",
       "3            1            0.0          1.0    N                 Y   \n",
       "4            1            3.0          4.0    N                 N   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                     Didn't deliver   \n",
       "1                    didn't work even the first time   \n",
       "2  pure junk, lowest quality you could possibly g...   \n",
       "3                   Cuisinart Keurig ruined my week!   \n",
       "4                           Not what I was expecting   \n",
       "\n",
       "                                         review_body review_date  class  \n",
       "0  Really didn't work as well as the pictures. Wh...  2014-12-01    2.0  \n",
       "1  brand new out of the box and it wouldn't even ...  2014-03-21    2.0  \n",
       "2  pure junk, lowest quality you could possibly g...  2015-07-07    2.0  \n",
       "3  Really bummed! Machine worked great until a fe...  2015-02-28    2.0  \n",
       "4  Sure, it cuts things, but the blades don't hav...  2007-02-20    2.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = pd.read_csv('binary_data.csv', index_col=0)\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Ready to dataset. We should change string(word) to number to feed tada into RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(temp):\n",
    "    # For each sentence, word tokenization is performed using NLTK\n",
    "    result = [word_tokenize(sentence) for sentence in temp]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my word2vec model\n",
    "word2vec = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the tokenized reviews to int type(using my Word2vec model)\n",
    "def review_to_int(reviews):\n",
    "    reviews_ints = []\n",
    "    for review in reviews:\n",
    "        # if specific word is in my word2vec model -> use index number. If not, put 0 instead of the words' index.\n",
    "        reviews_ints.append([word2vec.wv.key_to_index[word] if word in  word2vec.wv.key_to_index else 0 for word in review])\n",
    "        \n",
    "    return reviews_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the maximum review length to 50 \n",
    "def pad_features(x, desired_len):\n",
    "    for i, row in enumerate(x):\n",
    "        if len(row) > desired_len: # Turncate longer reviews\n",
    "            x[i] = row[:desired_len]\n",
    "        elif len(row) < desired_len: # Padding shorter reviews with a '0'\n",
    "            x[i] = row[:len(row)] + [0]*(desired_len-len(row))\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(new['review_body'], new['class'].values, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change words to number values using my own word2vec-news model\n",
    "new_x_train = review_to_int(tokenize(x_train))\n",
    "new_x_train = np.array(pad_features(new_x_train, 50))\n",
    "\n",
    "new_x_test = review_to_int(tokenize(x_test))\n",
    "new_x_test = np.array(pad_features(new_x_test, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# change data type to tensor\n",
    "X_train = torch.LongTensor(new_x_train)\n",
    "X_test = torch.LongTensor(new_x_test)\n",
    "Y_train = torch.LongTensor(y_train-1)\n",
    "Y_test = torch.LongTensor(y_test-1)\n",
    "\n",
    "# Make a dataset and dataloader\n",
    "ds_train = TensorDataset(X_train, Y_train)\n",
    "ds_test = TensorDataset(X_test, Y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_size, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, _ = self.rnn(embedded)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(word2vec.wv)+1\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, batch_size):\n",
    "    model.train()  # Model train\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Train model with mini batch\n",
    "    for data, targets in loader_train:\n",
    "        optimizer.zero_grad()  # Initiate        \n",
    "        outputs = model(data)  # model train and output\n",
    "        loss = loss_fn(outputs, targets.reshape(1,batch_size).t())  # Calculate loss values (real value - predicted value)\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Edit weights\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    print('Cost: {:.6f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    model.eval()  # Test Model\n",
    "    correct = 0\n",
    "\n",
    "    # Create minibatch\n",
    "    with torch.no_grad():  \n",
    "        for data, targets in loader_test:\n",
    "            outputs = model(data)  # Put input data and get output data\n",
    "            \n",
    "            # Calculate correct case\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Calculate which label has the highest probability\n",
    "            correct += predicted.eq(targets.data.view_as(predicted)).sum()  # If it matches the answer, increase the count\n",
    "                       \n",
    "    # Print accuracy\n",
    "    data_num = len(loader_test.dataset)\n",
    "    print('\\nAccuracy with test data: {}/{} ({:.0f}%)\\n'.format(correct,data_num, 100. * correct / data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.617980\n",
      "Cost: 0.593134\n",
      "Cost: 0.516981\n",
      "\n",
      "Accuracy with test data: 2630/4000 (66%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3): # I have computational resource limitations, especially memory issue, so I had to set epoch to 3. (epoch=3)\n",
    "    train(epoch, 64)\n",
    "test(model, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b-1-1) Binary model with GRU (a gated recurrent unit cell) using my word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_size, num_classes):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, _ = self.gru(embedded)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(word2vec.wv)+1\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.777114\n",
      "Cost: 0.658260\n",
      "Cost: 0.614778\n",
      "\n",
      "Accuracy with test data: 2562/4000 (64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3): \n",
    "    train(epoch, 64)\n",
    "test(model, loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a-1-2) Binary model with RNN using my google-word2vec-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec-google-news model\n",
    "import gensim.downloader as api\n",
    "google_wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the tokenized reviews to int type(using my Word2vec model)\n",
    "def google_review_to_int(reviews):\n",
    "    reviews_ints = []\n",
    "    for review in reviews:\n",
    "        # if specific word is in my word2vec model -> use index number. If not, put 0 instead of the words' index.\n",
    "        reviews_ints.append([google_wv.key_to_index[word] if word in  google_wv.key_to_index else 0 for word in review])\n",
    "        \n",
    "    return reviews_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change words to number values using google-word2vec-news model\n",
    "new_x_train = google_review_to_int(tokenize(x_train))\n",
    "new_x_train = np.array(pad_features(new_x_train, 50))\n",
    "\n",
    "new_x_test = google_review_to_int(tokenize(x_test))\n",
    "new_x_test = np.array(pad_features(new_x_test, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# change data type to tensor\n",
    "X_train = torch.LongTensor(new_x_train)\n",
    "X_test = torch.LongTensor(new_x_test)\n",
    "Y_train = torch.LongTensor(y_train-1)\n",
    "Y_test = torch.LongTensor(y_test-1)\n",
    "\n",
    "# Make a dataset and dataloader\n",
    "ds_train = TensorDataset(X_train, Y_train)\n",
    "ds_test = TensorDataset(X_test, Y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(google_wv)+1\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.693178\n",
      "Cost: 0.511630\n",
      "Cost: 0.574840\n",
      "\n",
      "Accuracy with test data: 2565/4000 (64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3): \n",
    "    train(epoch, 64)\n",
    "test(model, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b-1-2) Binary model with GRU (a gated recurrent unit cell) using google-word2vec-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(google_wv)+1\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.742343\n",
      "Cost: 0.570386\n",
      "Cost: 0.552954\n",
      "\n",
      "Accuracy with test data: 2625/4000 (66%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3): \n",
    "    train(epoch, 64)\n",
    "test(model, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a-2-1) Ternary model with RNN using my own word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>11903534</td>\n",
       "      <td>R3NBYY1PF9MXRT</td>\n",
       "      <td>B00FRMV38Y</td>\n",
       "      <td>872686823</td>\n",
       "      <td>Decodyne&amp;#0153; Morning Mug, Heat Sensitive Co...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Didn't deliver</td>\n",
       "      <td>Really didn't work as well as the pictures. Wh...</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>27885863</td>\n",
       "      <td>R1A10GP8CPG1A2</td>\n",
       "      <td>B003YFI0O6</td>\n",
       "      <td>111524501</td>\n",
       "      <td>Oster Electric Wine-Bottle Opener</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>didn't work even the first time</td>\n",
       "      <td>brand new out of the box and it wouldn't even ...</td>\n",
       "      <td>2014-03-21</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>15355157</td>\n",
       "      <td>R147NFWDLR0AT9</td>\n",
       "      <td>B0002T4ZL4</td>\n",
       "      <td>978772977</td>\n",
       "      <td>Oggi 5355 4-Piece Acrylic Canister Set with Ai...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>15647704</td>\n",
       "      <td>R1GQQLPV9LCY1T</td>\n",
       "      <td>B0034J6QIY</td>\n",
       "      <td>591197834</td>\n",
       "      <td>Cuisinart SS-700 Single Serve Brewing System -...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Cuisinart Keurig ruined my week!</td>\n",
       "      <td>Really bummed! Machine worked great until a fe...</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26424346</td>\n",
       "      <td>R1X5BB0UPZ4IWT</td>\n",
       "      <td>B000AXQA8I</td>\n",
       "      <td>330600737</td>\n",
       "      <td>Kuhn Rikon Twist and Chop, Artichoke</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Not what I was expecting</td>\n",
       "      <td>Sure, it cuts things, but the blades don't hav...</td>\n",
       "      <td>2007-02-20</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     11903534  R3NBYY1PF9MXRT  B00FRMV38Y       872686823   \n",
       "1          US     27885863  R1A10GP8CPG1A2  B003YFI0O6       111524501   \n",
       "2          US     15355157  R147NFWDLR0AT9  B0002T4ZL4       978772977   \n",
       "3          US     15647704  R1GQQLPV9LCY1T  B0034J6QIY       591197834   \n",
       "4          US     26424346  R1X5BB0UPZ4IWT  B000AXQA8I       330600737   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  Decodyne&#0153; Morning Mug, Heat Sensitive Co...          Kitchen   \n",
       "1                  Oster Electric Wine-Bottle Opener          Kitchen   \n",
       "2  Oggi 5355 4-Piece Acrylic Canister Set with Ai...          Kitchen   \n",
       "3  Cuisinart SS-700 Single Serve Brewing System -...          Kitchen   \n",
       "4               Kuhn Rikon Twist and Chop, Artichoke          Kitchen   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            1            0.0          0.0    N                 Y   \n",
       "1            1            1.0          1.0    N                 Y   \n",
       "2            1            2.0          2.0    N                 Y   \n",
       "3            1            0.0          1.0    N                 Y   \n",
       "4            1            3.0          4.0    N                 N   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                     Didn't deliver   \n",
       "1                    didn't work even the first time   \n",
       "2  pure junk, lowest quality you could possibly g...   \n",
       "3                   Cuisinart Keurig ruined my week!   \n",
       "4                           Not what I was expecting   \n",
       "\n",
       "                                         review_body review_date  class  \n",
       "0  Really didn't work as well as the pictures. Wh...  2014-12-01    2.0  \n",
       "1  brand new out of the box and it wouldn't even ...  2014-03-21    2.0  \n",
       "2  pure junk, lowest quality you could possibly g...  2015-07-07    2.0  \n",
       "3  Really bummed! Machine worked great until a fe...  2015-02-28    2.0  \n",
       "4  Sure, it cuts things, but the blades don't hav...  2007-02-20    2.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "new = pd.read_csv('ternary_data.csv', index_col=0)\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train data and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(new['review_body'], new['class'].values, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change words to number values using my own word2vec-news model\n",
    "new_x_train = review_to_int(tokenize(x_train))\n",
    "new_x_train = np.array(pad_features(new_x_train, 50))\n",
    "\n",
    "new_x_test = review_to_int(tokenize(x_test))\n",
    "new_x_test = np.array(pad_features(new_x_test, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# change data type to tensor\n",
    "X_train = torch.LongTensor(new_x_train)\n",
    "X_test = torch.LongTensor(new_x_test)\n",
    "Y_train = torch.LongTensor(y_train-1)\n",
    "Y_test = torch.LongTensor(y_test-1)\n",
    "\n",
    "# Make a dataset and dataloader\n",
    "ds_train = TensorDataset(X_train, Y_train)\n",
    "ds_test = TensorDataset(X_test, Y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(word2vec.wv)+1\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 1.055819\n",
      "Cost: 0.997521\n",
      "Cost: 0.919905\n",
      "\n",
      "Accuracy with test data: 2624/5000 (52%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    train(epoch, 32)\n",
    "test(model, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b-2-1) Ternary model with GRU (a gated recurrent unit cell) using my own word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(word2vec.wv)+1\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.891429\n",
      "Cost: 1.189794\n",
      "Cost: 0.805755\n",
      "\n",
      "Accuracy with test data: 2627/5000 (53%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    train(epoch, 32)\n",
    "test(model, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a-2-2) Ternary model with RNN using google-word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>11903534</td>\n",
       "      <td>R3NBYY1PF9MXRT</td>\n",
       "      <td>B00FRMV38Y</td>\n",
       "      <td>872686823</td>\n",
       "      <td>Decodyne&amp;#0153; Morning Mug, Heat Sensitive Co...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Didn't deliver</td>\n",
       "      <td>Really didn't work as well as the pictures. Wh...</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>27885863</td>\n",
       "      <td>R1A10GP8CPG1A2</td>\n",
       "      <td>B003YFI0O6</td>\n",
       "      <td>111524501</td>\n",
       "      <td>Oster Electric Wine-Bottle Opener</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>didn't work even the first time</td>\n",
       "      <td>brand new out of the box and it wouldn't even ...</td>\n",
       "      <td>2014-03-21</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>15355157</td>\n",
       "      <td>R147NFWDLR0AT9</td>\n",
       "      <td>B0002T4ZL4</td>\n",
       "      <td>978772977</td>\n",
       "      <td>Oggi 5355 4-Piece Acrylic Canister Set with Ai...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>pure junk, lowest quality you could possibly g...</td>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>15647704</td>\n",
       "      <td>R1GQQLPV9LCY1T</td>\n",
       "      <td>B0034J6QIY</td>\n",
       "      <td>591197834</td>\n",
       "      <td>Cuisinart SS-700 Single Serve Brewing System -...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Cuisinart Keurig ruined my week!</td>\n",
       "      <td>Really bummed! Machine worked great until a fe...</td>\n",
       "      <td>2015-02-28</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>26424346</td>\n",
       "      <td>R1X5BB0UPZ4IWT</td>\n",
       "      <td>B000AXQA8I</td>\n",
       "      <td>330600737</td>\n",
       "      <td>Kuhn Rikon Twist and Chop, Artichoke</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Not what I was expecting</td>\n",
       "      <td>Sure, it cuts things, but the blades don't hav...</td>\n",
       "      <td>2007-02-20</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     11903534  R3NBYY1PF9MXRT  B00FRMV38Y       872686823   \n",
       "1          US     27885863  R1A10GP8CPG1A2  B003YFI0O6       111524501   \n",
       "2          US     15355157  R147NFWDLR0AT9  B0002T4ZL4       978772977   \n",
       "3          US     15647704  R1GQQLPV9LCY1T  B0034J6QIY       591197834   \n",
       "4          US     26424346  R1X5BB0UPZ4IWT  B000AXQA8I       330600737   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  Decodyne&#0153; Morning Mug, Heat Sensitive Co...          Kitchen   \n",
       "1                  Oster Electric Wine-Bottle Opener          Kitchen   \n",
       "2  Oggi 5355 4-Piece Acrylic Canister Set with Ai...          Kitchen   \n",
       "3  Cuisinart SS-700 Single Serve Brewing System -...          Kitchen   \n",
       "4               Kuhn Rikon Twist and Chop, Artichoke          Kitchen   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            1            0.0          0.0    N                 Y   \n",
       "1            1            1.0          1.0    N                 Y   \n",
       "2            1            2.0          2.0    N                 Y   \n",
       "3            1            0.0          1.0    N                 Y   \n",
       "4            1            3.0          4.0    N                 N   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                                     Didn't deliver   \n",
       "1                    didn't work even the first time   \n",
       "2  pure junk, lowest quality you could possibly g...   \n",
       "3                   Cuisinart Keurig ruined my week!   \n",
       "4                           Not what I was expecting   \n",
       "\n",
       "                                         review_body review_date  class  \n",
       "0  Really didn't work as well as the pictures. Wh...  2014-12-01    2.0  \n",
       "1  brand new out of the box and it wouldn't even ...  2014-03-21    2.0  \n",
       "2  pure junk, lowest quality you could possibly g...  2015-07-07    2.0  \n",
       "3  Really bummed! Machine worked great until a fe...  2015-02-28    2.0  \n",
       "4  Sure, it cuts things, but the blades don't hav...  2007-02-20    2.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "new = pd.read_csv('ternary_data.csv', index_col=0)\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change words to number values using google-word2vec-news model\n",
    "new_x_train = google_review_to_int(tokenize(x_train))\n",
    "new_x_train = np.array(pad_features(new_x_train, 50))\n",
    "\n",
    "new_x_test = google_review_to_int(tokenize(x_test))\n",
    "new_x_test = np.array(pad_features(new_x_test, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# change data type to tensor\n",
    "X_train = torch.LongTensor(new_x_train)\n",
    "X_test = torch.LongTensor(new_x_test)\n",
    "Y_train = torch.LongTensor(y_train-1)\n",
    "Y_test = torch.LongTensor(y_test-1)\n",
    "\n",
    "# Make a dataset and dataloader\n",
    "ds_train = TensorDataset(X_train, Y_train)\n",
    "ds_test = TensorDataset(X_test, Y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(google_wv)+1\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 1.071386\n",
      "Cost: 0.833799\n",
      "Cost: 1.027805\n",
      "\n",
      "Accuracy with test data: 2630/5000 (53%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    train(epoch, 32)\n",
    "test(model, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b-2-2) Ternary model with GRU (a gated recurrent unit cell) using google-word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(google_wv)+1\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function -> CrossEntropyLoss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Select Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 1.103006\n",
      "Cost: 0.943143\n",
      "Cost: 1.102362\n",
      "\n",
      "Accuracy with test data: 2563/5000 (51%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    train(epoch, 32)\n",
    "test(model, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='5.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": The performance of RNN and GRU is not significantly different. However, I have computational resource limitations, especially memory issue, so I had to set epoch to 3. (epoch=3). If I have resources, I would like to increase the amount of epoch, which might get different results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
