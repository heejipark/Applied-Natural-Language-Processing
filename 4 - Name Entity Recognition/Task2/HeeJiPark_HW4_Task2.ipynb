{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbFZ2TuXwz3k"
   },
   "source": [
    "# Hee Ji Park (4090715830) - CSCI 544 HW4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR8maraew4z-"
   },
   "source": [
    "# Task2 : Using GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZkpkkxrFdxpB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gzip\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gJbgxPfZd91b"
   },
   "outputs": [],
   "source": [
    "# If the word is number, return True. Or return False\n",
    "def isNumber(s):\n",
    "    try:\n",
    "        if ',' in s: # ex) 4,800 -> 4800\n",
    "            s = s.replace(',','')\n",
    "        float(s) \n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "punct = set(string.punctuation) \n",
    "noun_suffix = [\"let\",'ie',\"kin\",\"action\", \"ling\", \"hood\", \"ship\", \"ary\",\"age\",\n",
    "               \"ery\", \"ory\", \"ance\", \"an\",\"ary\",\"eer\",\"er\",\"ier\",\"herd\",\"cy\", \"dom\", \n",
    "               \"ee\", \"ence\", \"ster\", \"yer\", \"ant\",\"ar\", \"ion\", \"ism\", \"ist\", \"ity\", \n",
    "               \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ize\", \"ise\"]\n",
    "adj_suffix = [\"able\", \"ible\", 'ive', \"ish\", \"ful\", 'ar', 'ary','ly','less','ic','ive','ous', \"ic\"]\n",
    "adv_suffix = [\"ly\",\"lng\",\"ward\", \"wards\", \"way\", \"ways\", \"wise\"]\n",
    "\n",
    "def unk_preprocessing(s):\n",
    "    # If unknown word has number, use this preprocessing\n",
    "    num = 0\n",
    "    for char in s:\n",
    "      if char.isdigit():\n",
    "         num += 1\n",
    "          \n",
    "    digitFraction = num / float(len(s))\n",
    "        \n",
    "    if s.isdigit(): #Is a digit\n",
    "        return \"<unk_num>\"\n",
    "    elif digitFraction > 0.5:\n",
    "        return \"<unk_mainly_num>\"\n",
    "    # If unknown word contains characteristics of verb, return <unk_verb> token\n",
    "    elif any(s.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"<unk_verb>\"\n",
    "    # If unknown word contains characteristics of adj, return <unk_adj> token\n",
    "    elif any(s.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"<unk_adj>\"\n",
    "    # If unknown word contains characteristics of adverbs, return <unk_adv> token\n",
    "    elif any(s.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"<unk_adv>\"\n",
    "    elif s.islower(): # All lower case\n",
    "        return \"<unk_all_lower>\"    \n",
    "    elif s.isupper(): # All upper case\n",
    "        return \"<unk_all_upper>\"              \n",
    "    elif s[0].isupper(): # If the first charter is upper case and then all lower\n",
    "        return \"<unk_initial_upper>\"\n",
    "    elif any(char.isdigit() for char in s): # if the word contains some number\n",
    "        return \"<unk_contain_num>\"    \n",
    "    else:\n",
    "        return \"<unk>\"\n",
    "\n",
    "def make_sequence(file, min_count=2):\n",
    "    vocab = {}\n",
    "    ner_set = set()\n",
    "    sentence = []\n",
    "    sentences = []\n",
    "    with open(file, \"r\") as train:\n",
    "        for line in train:\n",
    "            if not line.split(): # Ignore a blank line\n",
    "                sentences.append(sentence)\n",
    "                sentence =[]\n",
    "                continue\n",
    "            word_type, NER_type = line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
    "            if word_type not in vocab:\n",
    "                vocab[word_type] = 1\n",
    "            else:\n",
    "                vocab[word_type]+=1\n",
    "            sentence.append([word_type,NER_type])\n",
    "            ner_set.add(NER_type)\n",
    "        sentences.append(sentence)\n",
    "                \n",
    "        # make <unk> token\n",
    "        vocab['<unk>'], vocab['<unk_mainly_num>'] = 0,0\n",
    "        vocab['<unk_num>'], vocab['<unk_contain_num>'] = 0,0\n",
    "        vocab['<unk_verb>'], vocab['<unk_adj>'] = 0,0\n",
    "        vocab['<unk_adv>'], vocab['<unk_all_lower>'] = 0,0\n",
    "        vocab['<unk_all_upper>'], vocab['<unk_initial_upper>'] = 0,0\n",
    "        \n",
    "        delete = []\n",
    "        for word, occurrences in vocab.items():\n",
    "            if occurrences >= min_count: \n",
    "                continue\n",
    "            else:\n",
    "                new_token = unk_preprocessing(word)\n",
    "                vocab[new_token] += occurrences   # If occurrences is lower than 2 : change word name to < unk >\n",
    "                delete.append(word) # To remove the word in the dictionary (vocab), store 'word' in the delete list\n",
    "\n",
    "        for i in delete:  \n",
    "            del vocab[i] # Remove the word in the vocab dictionary\n",
    "    \n",
    "    return vocab, ner_set, sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GG5j9ItLeJ-8"
   },
   "outputs": [],
   "source": [
    "vocab, ner_set, sentences = make_sequence('./data/train')\n",
    "vocab_sorted = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "word_to_index = {w: i+1 for i, (w, n) in enumerate(vocab_sorted)}\n",
    "word_to_index['PAD'] = 0 # For Padding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9er50oE_eC5i"
   },
   "outputs": [],
   "source": [
    "ner_to_index = {}\n",
    "#ner_to_index['PAD'] = -100 # set padding = -100\n",
    "i = 0\n",
    "for ner in ner_set:\n",
    "    ner_to_index[ner] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FdPJ_92KeV6O"
   },
   "outputs": [],
   "source": [
    "# In order to change index to word\n",
    "index_to_word = {}\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "meTbc1tafEvJ"
   },
   "outputs": [],
   "source": [
    "# In order to change index to NER\n",
    "index_to_ner = {}\n",
    "for key, value in ner_to_index.items():\n",
    "    index_to_ner[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zFIm9uHeeStm"
   },
   "outputs": [],
   "source": [
    "# Make train input data\n",
    "data_X = []\n",
    "\n",
    "for s in sentences:\n",
    "    temp_X = []\n",
    "    for w, label in s:\n",
    "        if w in word_to_index:\n",
    "            temp_X.append(word_to_index.get(w))\n",
    "        else:\n",
    "            unk = unk_preprocessing(w)\n",
    "            temp_X.append(word_to_index[unk])\n",
    "    data_X.append(temp_X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PlgYOoUDePjd"
   },
   "outputs": [],
   "source": [
    "# Make train target data\n",
    "data_y = []\n",
    "for s in sentences:\n",
    "    temp_y = []\n",
    "    for w, label in s:\n",
    "        temp_y.append(ner_to_index.get(label))\n",
    "    data_y.append(temp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JBmjzyxPecqC"
   },
   "outputs": [],
   "source": [
    "# Limit the maximum review length to 130\n",
    "def pad_features_for_word(x, desired_len):\n",
    "    for i, row in enumerate(x):\n",
    "        if len(row) > desired_len: # Turncate longer reviews\n",
    "            x[i] = row[:desired_len]\n",
    "        elif len(row) < desired_len: # Padding shorter reviews with a '0'\n",
    "            x[i] = row[:len(row)] + [0]*(desired_len-len(row))\n",
    "        \n",
    "    return x\n",
    "# Limit the maximum review length to 130\n",
    "def pad_features_for_NER(x, desired_len):\n",
    "    for i, row in enumerate(x):\n",
    "        if len(row) > desired_len: # Turncate longer reviews\n",
    "            x[i] = row[:desired_len]\n",
    "        elif len(row) < desired_len: # Padding shorter reviews with a '0'\n",
    "            x[i] = row[:len(row)] + [-100]*(desired_len-len(row))\n",
    "        \n",
    "    return x\n",
    "\n",
    "# Padding and make dataset and dataloader\n",
    "data_X = pad_features_for_word(data_X, 130) \n",
    "data_y = pad_features_for_NER(data_y, 130)\n",
    "X_train = torch.LongTensor(data_X)\n",
    "Y_train = torch.LongTensor(data_y)\n",
    "ds_train = TensorDataset(X_train, Y_train)\n",
    "loader_train = DataLoader(ds_train, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lNIOAsB-eoyK"
   },
   "outputs": [],
   "source": [
    "# For GloVe word embedding\n",
    "with gzip.open('glove.6B.100d.gz', 'rb') as f_in:\n",
    "    with open('glove.6B.100d', 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "embedding_dict = dict()\n",
    "f = open(os.path.join('glove.6B.100d'), encoding='utf-8')\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') \n",
    "    embedding_dict[word] = word_vector_arr\n",
    "f.close()\n",
    "\n",
    "# Make word embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_to_index), embedding_dim))\n",
    "\n",
    "for word, i in word_to_index.items():\n",
    "    embedding_vector = embedding_dict.get(word.lower())\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix = torch.LongTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVeFUXnesFl",
    "outputId": "0a70c164-eb9b-4aba-ebb3-1026e7225354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# If a GPU is available, return True, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YUJo24Gdew0b"
   },
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, first_output_dim, output_dim, num_layers, bidirectional, drop_out): \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.blstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, bidirectional = bidirectional, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, first_output_dim)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.activation = nn.ELU()\n",
    "        self.fc2 = nn.Linear(first_output_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [sent len, batch size]\n",
    "        embedded = self.dropout(self.embedding(text)) # embedded = [batch size,sent len, emb dim]\n",
    "        outputs, (hidden, cell) = self.blstm(embedded) # output = [batch size, sent len , hid dim * n directions]\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.activation(self.fc1(outputs))\n",
    "        predictions = self.fc2(outputs) # predictions = [batch size, sent len, output dim]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmVWT8GFezG9",
    "outputId": "8540c781-358f-483a-c9c1-7fca9ed87b15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(word_to_index)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "FIRST_OUTPUT_DIM = 128\n",
    "OUTPUT_DIM = len(ner_to_index)\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.33\n",
    "\n",
    "model = BLSTM(INPUT_DIM, \n",
    "              EMBEDDING_DIM, \n",
    "              HIDDEN_DIM, \n",
    "              FIRST_OUTPUT_DIM,\n",
    "              OUTPUT_DIM, \n",
    "              N_LAYERS, \n",
    "              BIDIRECTIONAL, \n",
    "              DROPOUT)\n",
    "\n",
    "model.to(device)\n",
    "model.embedding.weight.data.copy_(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Im7LQhRhe049"
   },
   "outputs": [],
   "source": [
    "def model_train(model, iterator, predict_table):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_tot = 0\n",
    "    model.train()\n",
    "\n",
    "    for text, tags in iterator:\n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        tags = tags.to(device)\n",
    "        text = text.to(device)     \n",
    "        predictions = model(text)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1]) # #predictions = [sentence_len * batch size, output dim]\n",
    "        tags = tags.view(-1) # tags = [sentence_len * batch_size]\n",
    "        loss = criterion(predictions, tags)\n",
    "        tot, correct, predict_table = categorical_accuracy(predictions, tags, tag_pad_idx, text.view(-1), predict_table)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += correct\n",
    "        epoch_tot +=tot\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / epoch_tot, predict_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FrRzpEF6e6Ry"
   },
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx, text, predict_table):\n",
    "    tot = 0\n",
    "    correct = 0\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    for predict, real, word in zip(max_preds, y, text):\n",
    "        if real.item() == tag_pad_idx: # ignore padding\n",
    "            continue\n",
    "        else:\n",
    "            predict_table.append((word.item(), predict.item(), real.item()))\n",
    "            if real.item() == predict.item():\n",
    "                correct += 1\n",
    "            tot += 1\n",
    "    return tot, correct, predict_table\n",
    "\n",
    "def model_evaluate(model, iterator, predict_table):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_tot = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for text, tags in iterator:\n",
    "            tags = tags.to(device)\n",
    "            text = text.to(device)\n",
    "\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            tot, correct, predict_table = categorical_accuracy(predictions, tags, tag_pad_idx, text.view(-1), predict_table)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += correct\n",
    "            epoch_tot +=tot\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / epoch_tot, predict_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vdbmMq5me90J"
   },
   "outputs": [],
   "source": [
    "# For predicting dev file, make a sequence\n",
    "dev_sentences = []\n",
    "sentence=[]\n",
    "cnt=0\n",
    "with open('./data/dev', \"r\") as dev:\n",
    "    for line in dev:\n",
    "        if not line.split(): # Ignore a blank line\n",
    "            dev_sentences.append(sentence)\n",
    "            sentence =[]\n",
    "            continue\n",
    "        word_type, NER_type = line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
    "        cnt+=1\n",
    "        sentence.append([word_type,NER_type])\n",
    "    dev_sentences.append(sentence)\n",
    "\n",
    "dev_X = []\n",
    "for s in dev_sentences:\n",
    "    temp_X = []\n",
    "    for w, label in s:\n",
    "        if w in word_to_index:\n",
    "            temp_X.append(word_to_index.get(w))\n",
    "        else:\n",
    "            unk = unk_preprocessing(w) # if the word is not in vocab dictionary, change the word to unknown token\n",
    "            temp_X.append(word_to_index[unk])\n",
    "    dev_X.append(temp_X)\n",
    "\n",
    "dev_y = []\n",
    "for s in dev_sentences:\n",
    "    temp_y = []\n",
    "    for w, label in s:\n",
    "        temp_y.append(ner_to_index.get(label))\n",
    "    dev_y.append(temp_y)\n",
    "\n",
    "dev_X = pad_features_for_word(dev_X, 130) # Padding\n",
    "dev_y = pad_features_for_NER(dev_y, 130)\n",
    "\n",
    "X_dev = torch.LongTensor(dev_X)\n",
    "Y_dev = torch.LongTensor(dev_y)\n",
    "\n",
    "# Make a dataset and dataloader\n",
    "ds_dev = TensorDataset(X_dev, Y_dev)\n",
    "loader_dev = DataLoader(ds_dev, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNUglC5lfAN6",
    "outputId": "402caf83-e625-4a53-8fa0-77c0171f1842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.437 | Train Acc: 88.53%\n",
      "\t Val. Loss: 0.240 |  Val. Acc: 93.34%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.204 | Train Acc: 93.84%\n",
      "\t Val. Loss: 0.142 |  Val. Acc: 96.02%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.132 | Train Acc: 95.80%\n",
      "\t Val. Loss: 0.121 |  Val. Acc: 96.32%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.100 | Train Acc: 96.80%\n",
      "\t Val. Loss: 0.109 |  Val. Acc: 96.59%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.079 | Train Acc: 97.40%\n",
      "\t Val. Loss: 0.103 |  Val. Acc: 96.71%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.066 | Train Acc: 97.78%\n",
      "\t Val. Loss: 0.099 |  Val. Acc: 96.82%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.060 | Train Acc: 98.03%\n",
      "\t Val. Loss: 0.093 |  Val. Acc: 97.06%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.051 | Train Acc: 98.28%\n",
      "\t Val. Loss: 0.101 |  Val. Acc: 96.80%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.48%\n",
      "\t Val. Loss: 0.092 |  Val. Acc: 97.10%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.61%\n",
      "\t Val. Loss: 0.092 |  Val. Acc: 97.19%\n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.69%\n",
      "\t Val. Loss: 0.093 |  Val. Acc: 97.23%\n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.86%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.35%\n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.031 | Train Acc: 98.91%\n",
      "\t Val. Loss: 0.094 |  Val. Acc: 97.19%\n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.029 | Train Acc: 99.02%\n",
      "\t Val. Loss: 0.095 |  Val. Acc: 97.31%\n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.027 | Train Acc: 99.03%\n",
      "\t Val. Loss: 0.095 |  Val. Acc: 97.33%\n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.025 | Train Acc: 99.11%\n",
      "\t Val. Loss: 0.096 |  Val. Acc: 97.35%\n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.023 | Train Acc: 99.19%\n",
      "\t Val. Loss: 0.098 |  Val. Acc: 97.39%\n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.020 | Train Acc: 99.27%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.64%\n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.019 | Train Acc: 99.32%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.66%\n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.018 | Train Acc: 99.35%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.67%\n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.018 | Train Acc: 99.35%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.68%\n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.018 | Train Acc: 99.35%\n",
      "\t Val. Loss: 0.089 |  Val. Acc: 97.68%\n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.018 | Train Acc: 99.36%\n",
      "\t Val. Loss: 0.091 |  Val. Acc: 97.65%\n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.017 | Train Acc: 99.39%\n",
      "\t Val. Loss: 0.091 |  Val. Acc: 97.70%\n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.017 | Train Acc: 99.40%\n",
      "\t Val. Loss: 0.091 |  Val. Acc: 97.69%\n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.017 | Train Acc: 99.39%\n",
      "\t Val. Loss: 0.092 |  Val. Acc: 97.65%\n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.092 |  Val. Acc: 97.64%\n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.091 |  Val. Acc: 97.69%\n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.43%\n",
      "\t Val. Loss: 0.091 |  Val. Acc: 97.71%\n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.72%\n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.44%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.72%\n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.43%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.44%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.72%\n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.72%\n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.72%\n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.43%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.44%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.44%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.44%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.41%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.43%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.45%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.45%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.44%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 51\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.44%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 52\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.41%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 53\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.43%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 54\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.45%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 55\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.44%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 56\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 57\n",
      "\tTrain Loss: 0.015 | Train Acc: 99.45%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 58\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 59\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.42%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n",
      "Epoch: 60\n",
      "\tTrain Loss: 0.016 | Train Acc: 99.45%\n",
      "\t Val. Loss: 0.090 |  Val. Acc: 97.71%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 60\n",
    "tag_pad_idx=-100\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.23, momentum=0.9, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= -100)\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_predict_table = []\n",
    "    test_predict_table = []\n",
    "\n",
    "    train_loss, train_acc, train_predict_table = model_train(model, loader_train, train_predict_table)\n",
    "    valid_loss, valid_acc, valid_predict_table = model_evaluate(model, loader_dev, test_predict_table)\n",
    "\n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_predict_table = valid_predict_table\n",
    "        torch.save(model.state_dict(), './result/blstm2.pt')\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "exff4KMJfVlj"
   },
   "outputs": [],
   "source": [
    "# Fungtions for evaluate \n",
    "def categorical_evaluate(preds, text, predict_table):\n",
    "\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # Get the index of the max probability\n",
    "    for predict, word in zip(max_preds, text):\n",
    "        if word == 0:\n",
    "            continue\n",
    "        else:\n",
    "            predict_table.append((word, predict[0]))\n",
    "\n",
    "    return predict_table\n",
    "\n",
    "def model_evaluate(model, iterator, predict_table):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_tot = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in iterator:\n",
    "            text = text.to(device)\n",
    "            predictions = model(text)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            predict_table = categorical_evaluate(predictions, text.view(-1), predict_table)\n",
    "\n",
    "    return predict_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRybqID9W0u5"
   },
   "source": [
    "# Dev Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "agccoal3fRM7"
   },
   "outputs": [],
   "source": [
    "# Predict Dev Set and make dev2.out file\n",
    "term = [int(x[0]) for x in best_predict_table]\n",
    "y_pred = [int(x[1]) for x in best_predict_table]\n",
    "i=0\n",
    "newfile = open('./result/dev2.out', \"w\")\n",
    "with open('./data/dev', \"r\") as train:\n",
    "    for line in train:\n",
    "        if not line.split(): # Ignore a blank line\n",
    "            newfile.write('\\n')\n",
    "            continue\n",
    "        index, word_type = line.split(\" \")[0], line.split(\" \")[1].strip('\\n')\n",
    "        newfile.write(str(index)+' '+str(word_type)+' '+str(index_to_ner[y_pred[i]])+'\\n')\n",
    "        i += 1\n",
    "newfile.close()\n",
    "\n",
    "i=0\n",
    "newfile = open('./result/dev2_for_perl.out', \"w\")\n",
    "with open('./data/dev', \"r\") as train:\n",
    "    for line in train:\n",
    "        if not line.split(): # Ignore a blank line\n",
    "            newfile.write('\\n')\n",
    "            continue\n",
    "        index, word_type, NER_type = line.split(\" \")[0], line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
    "        newfile.write(str(index)+' '+str(word_type)+' '+str(NER_type)+' '+str(index_to_ner[y_pred[i]])+'\\n')\n",
    "        i += 1\n",
    "newfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaLfAwIgWvFx"
   },
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "m-s08dPJdJJd"
   },
   "outputs": [],
   "source": [
    "# Predict test set\n",
    "test_X = []\n",
    "sentence = []\n",
    "cnt=0\n",
    "with open('./data/test', \"r\") as test:\n",
    "    for line in test:\n",
    "        if not line.split(): # Ignore a blank line\n",
    "            test_X.append(sentence)\n",
    "            sentence = []\n",
    "            continue\n",
    "        word_type = line.split(\" \")[1]\n",
    "        if word_type in word_to_index:\n",
    "            sentence.append(word_to_index.get(word_type))\n",
    "        else:\n",
    "            unk = unk_preprocessing(word_type) # if the word is not in vocab dictionary, change the word to unknown token\n",
    "            sentence.append(word_to_index.get(unk))\n",
    "    test_X.append(sentence)\n",
    "\n",
    "test_X = pad_features_for_word(test_X, 130) # Padding\n",
    "X_test = torch.LongTensor(test_X)\n",
    "loader_test = DataLoader(X_test, batch_size=16, shuffle=False)\n",
    "\n",
    "evaluate_predict_table2 = []\n",
    "model = BLSTM(INPUT_DIM, \n",
    "              EMBEDDING_DIM, \n",
    "              HIDDEN_DIM, \n",
    "              FIRST_OUTPUT_DIM,\n",
    "              OUTPUT_DIM, \n",
    "              N_LAYERS, \n",
    "              BIDIRECTIONAL, \n",
    "              DROPOUT)\n",
    "model.to(device)\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "model.load_state_dict(torch.load('./result/blstm2.pt')) # load pretrained model\n",
    "prediction_table = model_evaluate(model, loader_test, evaluate_predict_table2)\n",
    "\n",
    "term = [int(x[0]) for x in evaluate_predict_table2]\n",
    "y_pred = [int(x[1]) for x in evaluate_predict_table2]\n",
    "\n",
    "# Make test2.out file\n",
    "i=0\n",
    "newfile = open('./result/test2.out', \"w\")\n",
    "with open('./data/test', \"r\") as test:\n",
    "    for line in test:\n",
    "        if not line.split(): # Ignore a blank line\n",
    "            newfile.write('\\n')\n",
    "            continue\n",
    "        index, word_type = line.split(\" \")[0], line.split(\" \")[1].strip('\\n')\n",
    "        for_tag = index_to_ner[y_pred[i]]\n",
    "        newfile.write(str(index)+' '+str(word_type)+' '+for_tag+'\\n')\n",
    "        i += 1\n",
    "newfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PAryYL0Gjb0C"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save data\n",
    "with open('./data/vocab_dictionary.pickle','wb') as fw1:\n",
    "    pickle.dump(word_to_index, fw1)\n",
    "with open('./data/ner_dictionary.pickle','wb') as fw2:\n",
    "    pickle.dump(ner_to_index, fw2)\n",
    "with open('./data/int_vocab_dictionary.pickle','wb') as fw3:\n",
    "    pickle.dump(index_to_word, fw3)\n",
    "with open('./data/int_ner_dictionary.pickle','wb') as fw4:\n",
    "    pickle.dump(index_to_ner, fw4)\n",
    "with open('./data/loader_train.pickle','wb') as fw5:\n",
    "    pickle.dump(loader_train, fw5)\n",
    "with open('./data/loader_dev.pickle','wb') as fw6:\n",
    "    pickle.dump(loader_dev, fw6)\n",
    "with open('./data/loader_test.pickle','wb') as fw7:\n",
    "    pickle.dump(loader_test, fw7)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Cjw3qjrFtU_B"
   },
   "outputs": [],
   "source": [
    "with open('./data/embedding_matrix.pickle','wb') as fw8:\n",
    "    pickle.dump(embedding_matrix, fw8)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "O4M3NaChnwBK"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load data\n",
    "with open('./data/vocab_dictionary.pickle', 'rb') as fr1:\n",
    "    word_to_index = pickle.load(fr1)\n",
    "with open('./data/ner_dictionary.pickle', 'rb') as fr2:\n",
    "    index_to_word = pickle.load(fr2)\n",
    "with open('./data/int_vocab_dictionary.pickle', 'rb') as fr3:\n",
    "    ner_to_index = pickle.load(fr3)\n",
    "with open('./data/int_ner_dictionary.pickle', 'rb') as fr4:\n",
    "    index_to_ner = pickle.load(fr4)\n",
    "with open('./data/loader_train.pickle', 'rb') as fr5:\n",
    "    loader_train = pickle.load(fr5)\n",
    "with open('./data/loader_dev.pickle', 'rb') as fr6:\n",
    "    loader_dev = pickle.load(fr6)\n",
    "with open('./data/loader_test.pickle', 'rb') as fr7:\n",
    "    loader_test = pickle.load(fr7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CzwdJUXZEJOe"
   },
   "outputs": [],
   "source": [
    "checkpoint = {'INPUT_DIM':len(word_to_index),\n",
    "              'EMBEDDING_DIM':100,\n",
    "              'HIDDEN_DIM':256,\n",
    "              'FIRST_OUTPUT_DIM':128,\n",
    "              'OUTPUT_DIM':len(ner_to_index),\n",
    "              'N_LAYERS':1,\n",
    "              'BIDIRECTIONAL':True,\n",
    "              'DROPOUT':0.33,\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'result/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXBOD4WvEKED"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HeeJiPark_HW4_Task2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
