{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HeeJiPark_HW4_Task1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piIoZrNwbpp0"
      },
      "source": [
        "# Hee Ji Park (4090715830) - CSCI HW4 - Task1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjaA5T22ieqx"
      },
      "source": [
        "# Task1 - Simple Bidirectional LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QacD7ddieqy"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUUeKM5x_EjD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNmktOnQieq0"
      },
      "source": [
        "## Preprocessing for unknown words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zmh5oGOG_Rjt"
      },
      "source": [
        "# If the word is number, return True. Or return False\n",
        "def isNumber(s):\n",
        "    try:\n",
        "        if ',' in s: # ex) 4,800 -> 4800\n",
        "            s = s.replace(',','')\n",
        "        float(s) \n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX1Xg9rb_S0I"
      },
      "source": [
        "# This code is to classify unknown words\n",
        "punct = set(string.punctuation) \n",
        "noun_suffix = [\"let\",'ie',\"kin\",\"action\", \"ling\", \"hood\", \"ship\", \"ary\",\"age\",\n",
        "               \"ery\", \"ory\", \"ance\", \"an\",\"ary\",\"eer\",\"er\",\"ier\",\"herd\",\"cy\", \"dom\", \n",
        "               \"ee\", \"ence\", \"ster\", \"yer\", \"ant\",\"ar\", \"ion\", \"ism\", \"ist\", \"ity\", \n",
        "               \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ty\"]\n",
        "verb_suffix = [\"ate\", \"ify\", \"ize\", \"ise\"]\n",
        "adj_suffix = [\"able\", \"ible\", 'ant', 'ent', 'ive', \"al\",\"ial\",\"an\",\"ian\",\"ish\",\n",
        "              \"ern\", \"ese\", \"ful\", 'ar', 'ary','ly','less','ic','ive','ous', \"i\", \"ic\"]\n",
        "adv_suffix = [\"ly\",\"lng\",\"ward\", \"wards\", \"way\", \"ways\", \"wise\"]\n",
        "\n",
        "def unk_preprocessing(s):\n",
        "    # If unknown word has number, return <unk_num> token\n",
        "    num = 0\n",
        "    for char in s:\n",
        "        if char.isdigit():\n",
        "            num += 1\n",
        "          \n",
        "    digitFraction = num / float(len(s))\n",
        "        \n",
        "    if s.isdigit(): #Is a digit\n",
        "        return \"<unk_num>\"\n",
        "    elif digitFraction > 0.5:\n",
        "        return \"<unk_mainly_num>\"\n",
        "    # If unknown word contains characteristics of verb, return <unk_verb> token\n",
        "    elif any(s.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"<unk_verb>\"\n",
        "    # If unknown word contains characteristics of adj, return <unk_adj> token\n",
        "    elif any(s.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"<unk_adj>\"\n",
        "    # If unknown word contains characteristics of adverbs, return <unk_adv> token\n",
        "    elif any(s.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"<unk_adv>\"\n",
        "    elif s.islower(): #All lower case\n",
        "        return \"<unk_all_lower>\"    \n",
        "    elif s.isupper(): #All upper case\n",
        "        return \"<unk_all_upper>\"              \n",
        "    elif s[0].isupper(): #is a title, initial char upper, then all lower\n",
        "        return \"<unk_initial_upper>\"\n",
        "    elif any(char.isdigit() for char in s):\n",
        "        return \"<unk_contain_num>\"    \n",
        "    else:\n",
        "        return \"<unk>\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19EVncZPieq5"
      },
      "source": [
        "## Make a vocabulary and datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RUQZatX_UXG"
      },
      "source": [
        "# Make a vocabulary for input data\n",
        "def make_sequence(file, min_count=2):\n",
        "    vocab = {}\n",
        "    ner_set = set()\n",
        "    sentence = []\n",
        "    sentences = []\n",
        "    with open(file, \"r\") as train:\n",
        "        for line in train:\n",
        "            if not line.split(): # Ignore a blank line\n",
        "                sentences.append(sentence)\n",
        "                sentence =[]\n",
        "                continue\n",
        "            word_type, NER_type = line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
        "            if word_type not in vocab:\n",
        "                vocab[word_type] = 1\n",
        "            else:\n",
        "                vocab[word_type]+=1\n",
        "            sentence.append([word_type,NER_type])\n",
        "            ner_set.add(NER_type)\n",
        "        sentences.append(sentence)\n",
        "                \n",
        "        # make <unk> token\n",
        "        vocab['<unk>'], vocab['<unk_mainly_num>'] = 0,0\n",
        "        vocab['<unk_num>'], vocab['<unk_contain_num>'] = 0,0\n",
        "        vocab['<unk_verb>'], vocab['<unk_adj>'] = 0,0\n",
        "        vocab['<unk_adv>'], vocab['<unk_all_lower>'] = 0,0\n",
        "        vocab['<unk_all_upper>'], vocab['<unk_initial_upper>'] = 0,0\n",
        "        \n",
        "\n",
        "        delete = []\n",
        "        for word, occurrences in vocab.items():\n",
        "            if occurrences >= min_count: \n",
        "                continue\n",
        "            else:\n",
        "                new_token = unk_preprocessing(word)\n",
        "                vocab[new_token] += occurrences   # If occurrences is lower than 3 : change word name to < unk >\n",
        "                delete.append(word) # To remove the word in the dictionary (vocab), store 'word' in the delete list\n",
        "\n",
        "        for i in delete:  \n",
        "            del vocab[i] # Remove the word in the vocab dictionary\n",
        "    \n",
        "    return vocab, ner_set, sentences"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_BBZdl6_V1I"
      },
      "source": [
        "vocab, ner_set, sentences = make_sequence('./data/train')\n",
        "vocab_sorted = sorted(vocab.items(), key=lambda x:x[1], reverse=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCbR6TX3_Xow"
      },
      "source": [
        "# Make a dictionary\n",
        "word_to_index = {w: i+1 for i, (w, n) in enumerate(vocab_sorted)}\n",
        "word_to_index['PAD'] = 0 # This is for padding words"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewquyX60_Y1O",
        "outputId": "097f97d8-6ae0-4352-d986-7c4336f3c146"
      },
      "source": [
        "# Make NER to dictionary. This is for changing the NER tags to number\n",
        "ner_to_index = {}\n",
        "i = 0\n",
        "for ner in ner_set:\n",
        "    ner_to_index[ner] = i\n",
        "    i += 1\n",
        "print(ner_to_index)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I-LOC': 0, 'I-MISC': 1, 'B-LOC': 2, 'I-ORG': 3, 'B-ORG': 4, 'B-PER': 5, 'O': 6, 'I-PER': 7, 'B-MISC': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yEPbaML_bym"
      },
      "source": [
        "# Dictionary: Index to word\n",
        "index_to_word = {}\n",
        "for key, value in word_to_index.items():\n",
        "    index_to_word[value] = key"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjM1w-iVjpIx"
      },
      "source": [
        "# Change index to NER\n",
        "index_to_ner = {}\n",
        "for key, value in ner_to_index.items():\n",
        "    index_to_ner[value] = key"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e81BxShr_aGC"
      },
      "source": [
        "# This code is for input sequence\n",
        "data_X = []\n",
        "\n",
        "for s in sentences:\n",
        "    temp_X = []\n",
        "    for w, label in s:\n",
        "        if w in word_to_index:\n",
        "            temp_X.append(word_to_index.get(w))\n",
        "        else:\n",
        "            unk = unk_preprocessing(w)\n",
        "            temp_X.append(word_to_index[unk])\n",
        "    data_X.append(temp_X)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg_mxcgD_dwY"
      },
      "source": [
        "# This code is for target sequence\n",
        "data_y = []\n",
        "for s in sentences:\n",
        "    temp_y = []\n",
        "    for w, label in s:\n",
        "        temp_y.append(ner_to_index.get(label))\n",
        "    data_y.append(temp_y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYoOQ7OX_fQo"
      },
      "source": [
        "# Limit the maximum review length to 130 \n",
        "def pad_features_for_word(x, desired_len):\n",
        "    for i, row in enumerate(x):\n",
        "        if len(row) > desired_len: # Turncate longer sentences\n",
        "            x[i] = row[:desired_len]\n",
        "        elif len(row) < desired_len: # Padding shorter sentencess with a '0'\n",
        "            x[i] = row[:len(row)] + [0]*(desired_len-len(row))\n",
        "        \n",
        "    return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jcMIQV0xr0Z"
      },
      "source": [
        "# Limit the maximum review length to 130 \n",
        "def pad_features_for_NER(x, desired_len):\n",
        "    for i, row in enumerate(x):\n",
        "        if len(row) > desired_len: # Turncate longer sentences\n",
        "            x[i] = row[:desired_len]\n",
        "        elif len(row) < desired_len: # Padding shorter sentencess with a '-100'\n",
        "            x[i] = row[:len(row)] + [-100]*(desired_len-len(row))\n",
        "        \n",
        "    return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8z6EMZz_hKo"
      },
      "source": [
        "# Make a dataset and dataloader\n",
        "data_X = pad_features_for_word(data_X, 130)\n",
        "data_y = pad_features_for_NER(data_y, 130)\n",
        "\n",
        "X_train = torch.LongTensor(data_X)\n",
        "Y_train = torch.LongTensor(data_y)\n",
        "\n",
        "ds_train = TensorDataset(X_train, Y_train)\n",
        "loader_train = DataLoader(ds_train, batch_size=10, shuffle=False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNEI6hLrierM"
      },
      "source": [
        "## Set GPU or CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nA8oLp_P_mCi",
        "outputId": "da38cd96-d564-4aa9-9f82-2814bcacc579"
      },
      "source": [
        "# If a GPU is available, return True. Else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# Set CPU or GPU\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2UAj5kPierN"
      },
      "source": [
        "## BLSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAQh7nC6A-OI"
      },
      "source": [
        "class BLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, first_output_dim, output_dim, num_layers, bidirectional, drop_out): \n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.blstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, bidirectional = bidirectional, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, first_output_dim)\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        self.activation = nn.ELU()\n",
        "        self.fc2 = nn.Linear(first_output_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text = [batch size, sentence length]\n",
        "        embedded = self.dropout(self.embedding(text)) # embedded = [batch size, sentence length, embedding dim]\n",
        "        outputs, (hidden, cell) = self.blstm(embedded) # output = [batch size, sentence length , hidden dim * n_layers directions]\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.activation(self.fc1(outputs))\n",
        "        predictions = self.fc2(outputs) # predictions = [batch size, sentence length, output dim]\n",
        "        return predictions"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcHkZBxa_p8d",
        "outputId": "ab33fcbe-46eb-40cd-b20b-6ecc63efbfe0"
      },
      "source": [
        "# Model BLSTM\n",
        "INPUT_DIM = len(word_to_index)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "FIRST_OUTPUT_DIM = 128\n",
        "OUTPUT_DIM = len(ner_to_index)\n",
        "N_LAYERS = 1\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.33\n",
        "\n",
        "model = BLSTM(INPUT_DIM, \n",
        "              EMBEDDING_DIM, \n",
        "              HIDDEN_DIM, \n",
        "              FIRST_OUTPUT_DIM,\n",
        "              OUTPUT_DIM, \n",
        "              N_LAYERS, \n",
        "              BIDIRECTIONAL, \n",
        "              DROPOUT)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLSTM(\n",
              "  (embedding): Embedding(11994, 100, padding_idx=0)\n",
              "  (blstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
              "  (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (dropout): Dropout(p=0.33, inplace=False)\n",
              "  (activation): ELU(alpha=1.0)\n",
              "  (fc2): Linear(in_features=128, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee-dOusZierO"
      },
      "source": [
        "## Train and Test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZdLOrP7Bh4t"
      },
      "source": [
        "def model_train(model, iterator, predict_table):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_tot = 0\n",
        "    model.train()\n",
        "\n",
        "    for text, tags in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        tags = tags.to(device)\n",
        "        text = text.to(device)   \n",
        "        predictions = model(text)\n",
        "        predictions = predictions.view(-1, predictions.shape[-1]) # #predictions = [batch size * sentence length, output dim]\n",
        "        tags = tags.view(-1) # tags = [batch_size * sentence length]\n",
        "\n",
        "        loss = criterion(predictions, tags)\n",
        "\n",
        "        tot, correct, predict_table = categorical_accuracy(predictions, tags, tag_pad_idx, text.view(-1), predict_table)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += correct\n",
        "        epoch_tot +=tot\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / epoch_tot, predict_table"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls2WMT9CBisz"
      },
      "source": [
        "def model_evaluate(model, iterator, predict_table):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_tot = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for text, tags in iterator:\n",
        "            tags = tags.to(device)\n",
        "            text = text.to(device)\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "            \n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            tot, correct, predict_table = categorical_accuracy(predictions, tags, tag_pad_idx, text.view(-1), predict_table)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += correct\n",
        "            epoch_tot +=tot\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / epoch_tot, predict_table"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwWVPwTRBi8L"
      },
      "source": [
        "def categorical_accuracy(preds, y, tag_pad_idx, text, predict_table):\n",
        "    tot = 0\n",
        "    correct = 0\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # Get the index of the max probability\n",
        "    for predict, real, word in zip(max_preds, y, text):\n",
        "        if real.item() == tag_pad_idx: # ignore padding index\n",
        "            continue\n",
        "        else:\n",
        "            predict_table.append((word.item(), predict.item(), real.item()))\n",
        "            if real.item() == predict.item():\n",
        "                correct += 1\n",
        "            tot += 1\n",
        "    return tot, correct, predict_table"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuRcMJ7UBjJr"
      },
      "source": [
        "# This code is for dev dataset\n",
        "dev_sentences = []\n",
        "sentence=[]\n",
        "cnt=0\n",
        "with open('./data/dev', \"r\") as dev:\n",
        "    for line in dev:\n",
        "        if not line.split(): # Ignore a blank line\n",
        "            dev_sentences.append(sentence)\n",
        "            sentence =[]\n",
        "            continue\n",
        "        word_type, NER_type = line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
        "        cnt+=1\n",
        "        sentence.append([word_type,NER_type])\n",
        "    dev_sentences.append(sentence)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FADrrOWLBjUf"
      },
      "source": [
        "# Make dev dataset\n",
        "dev_X = []\n",
        "\n",
        "for s in dev_sentences:\n",
        "    temp_X = []\n",
        "    for w, label in s:\n",
        "        if w in word_to_index:\n",
        "            temp_X.append(word_to_index.get(w))\n",
        "        else:\n",
        "            unk = unk_preprocessing(w)\n",
        "            temp_X.append(word_to_index[unk])\n",
        "    dev_X.append(temp_X)\n",
        "\n",
        "dev_y = []\n",
        "for s in dev_sentences:\n",
        "    temp_y = []\n",
        "    for w, label in s:\n",
        "        temp_y.append(ner_to_index.get(label))\n",
        "    dev_y.append(temp_y)\n",
        "\n",
        "dev_X = pad_features_for_word(dev_X, 130)\n",
        "dev_y = pad_features_for_NER(dev_y, 130)\n",
        "X_dev = torch.LongTensor(dev_X)\n",
        "Y_dev = torch.LongTensor(dev_y)\n",
        "\n",
        "# Make a dataset and dataloader\n",
        "ds_dev = TensorDataset(X_dev, Y_dev)\n",
        "loader_dev = DataLoader(ds_dev, batch_size=10, shuffle=False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fApQA2ytix9s"
      },
      "source": [
        "import pickle\n",
        "# save data\n",
        "with open('./data/vocab_dictionary.pickle','wb') as fw1:\n",
        "    pickle.dump(word_to_index, fw1)\n",
        "with open('./data/ner_dictionary.pickle','wb') as fw2:\n",
        "    pickle.dump(ner_to_index, fw2)\n",
        "with open('./data/int_vocab_dictionary.pickle','wb') as fw3:\n",
        "    pickle.dump(index_to_word, fw3)\n",
        "with open('./data/int_ner_dictionary.pickle','wb') as fw4:\n",
        "    pickle.dump(index_to_ner, fw4)\n",
        "with open('./data/loader_train.pickle','wb') as fw5:\n",
        "    pickle.dump(loader_train, fw5)\n",
        "with open('./data/loader_dev.pickle','wb') as fw6:\n",
        "    pickle.dump(loader_dev, fw6)\n",
        "    "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22wbHMbgierR"
      },
      "source": [
        "## Train and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGGvl9rBNFUo",
        "outputId": "e87c8d6b-2e3d-4c9e-aea1-a72bf7da8c6f"
      },
      "source": [
        "# epoch - Train and evaluation\n",
        "N_EPOCHS = 20\n",
        "tag_pad_idx=-100\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True) # Set hyperparameter\n",
        "criterion = nn.CrossEntropyLoss(ignore_index= -100)\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    train_predict_table = []\n",
        "    test_predict_table = []\n",
        "\n",
        "    train_loss, train_acc, train_predict_table = model_train(model, loader_train, train_predict_table)\n",
        "    valid_loss, valid_acc, valid_predict_table = model_evaluate(model, loader_dev, test_predict_table)\n",
        "\n",
        "    if valid_loss <= best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        best_predict_table = valid_predict_table\n",
        "        torch.save(model.state_dict(), './result/blstm1.pt')\n",
        "        \n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.652 | Train Acc: 84.97%\n",
            "\t Val. Loss: 0.438 |  Val. Acc: 88.31%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.446 | Train Acc: 87.96%\n",
            "\t Val. Loss: 0.303 |  Val. Acc: 91.29%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.350 | Train Acc: 89.85%\n",
            "\t Val. Loss: 0.248 |  Val. Acc: 92.62%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.298 | Train Acc: 90.98%\n",
            "\t Val. Loss: 0.210 |  Val. Acc: 93.74%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.262 | Train Acc: 91.70%\n",
            "\t Val. Loss: 0.188 |  Val. Acc: 94.31%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.236 | Train Acc: 92.37%\n",
            "\t Val. Loss: 0.174 |  Val. Acc: 94.64%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.214 | Train Acc: 93.00%\n",
            "\t Val. Loss: 0.167 |  Val. Acc: 94.91%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.203 | Train Acc: 93.26%\n",
            "\t Val. Loss: 0.155 |  Val. Acc: 95.23%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.188 | Train Acc: 93.66%\n",
            "\t Val. Loss: 0.154 |  Val. Acc: 95.34%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.178 | Train Acc: 93.99%\n",
            "\t Val. Loss: 0.145 |  Val. Acc: 95.60%\n",
            "Epoch: 11\n",
            "\tTrain Loss: 0.171 | Train Acc: 94.14%\n",
            "\t Val. Loss: 0.147 |  Val. Acc: 95.60%\n",
            "Epoch: 12\n",
            "\tTrain Loss: 0.161 | Train Acc: 94.43%\n",
            "\t Val. Loss: 0.140 |  Val. Acc: 95.80%\n",
            "Epoch: 13\n",
            "\tTrain Loss: 0.154 | Train Acc: 94.59%\n",
            "\t Val. Loss: 0.139 |  Val. Acc: 95.79%\n",
            "Epoch: 14\n",
            "\tTrain Loss: 0.147 | Train Acc: 94.84%\n",
            "\t Val. Loss: 0.136 |  Val. Acc: 95.87%\n",
            "Epoch: 15\n",
            "\tTrain Loss: 0.141 | Train Acc: 95.01%\n",
            "\t Val. Loss: 0.139 |  Val. Acc: 95.76%\n",
            "Epoch: 16\n",
            "\tTrain Loss: 0.138 | Train Acc: 95.12%\n",
            "\t Val. Loss: 0.133 |  Val. Acc: 96.06%\n",
            "Epoch: 17\n",
            "\tTrain Loss: 0.133 | Train Acc: 95.28%\n",
            "\t Val. Loss: 0.129 |  Val. Acc: 96.09%\n",
            "Epoch: 18\n",
            "\tTrain Loss: 0.127 | Train Acc: 95.40%\n",
            "\t Val. Loss: 0.128 |  Val. Acc: 96.07%\n",
            "Epoch: 19\n",
            "\tTrain Loss: 0.127 | Train Acc: 95.47%\n",
            "\t Val. Loss: 0.130 |  Val. Acc: 96.02%\n",
            "Epoch: 20\n",
            "\tTrain Loss: 0.120 | Train Acc: 95.67%\n",
            "\t Val. Loss: 0.129 |  Val. Acc: 96.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMqw6qoTierT"
      },
      "source": [
        "## Dev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xKMITt6auV_"
      },
      "source": [
        "# Save the result as a '.out' file\n",
        "term = [int(x[0]) for x in best_predict_table]\n",
        "y_pred = [int(x[1]) for x in best_predict_table]\n",
        "i=0\n",
        "newfile = open('./result/dev1.out', \"w\")\n",
        "with open('./data/dev', \"r\") as train:\n",
        "    for line in train:\n",
        "        if not line.split(): # Ignore a blank line\n",
        "            newfile.write('\\n')\n",
        "            continue\n",
        "        index, word_type = line.split(\" \")[0], line.split(\" \")[1].strip('\\n')\n",
        "        newfile.write(str(index)+' '+str(word_type)+' '+str(index_to_ner[y_pred[i]])+'\\n')\n",
        "        i += 1\n",
        "newfile.close()\n",
        "\n",
        "i=0\n",
        "newfile = open('./result/dev1_for_perl.out', \"w\")\n",
        "with open('./data/dev', \"r\") as train:\n",
        "    for line in train:\n",
        "        if not line.split(): # Ignore a blank line\n",
        "            newfile.write('\\n')\n",
        "            continue\n",
        "        index, word_type, NER_type = line.split(\" \")[0], line.split(\" \")[1], line.split(\" \")[2].strip('\\n')\n",
        "        newfile.write(str(index)+' '+str(word_type)+' '+str(NER_type)+' '+str(index_to_ner[y_pred[i]])+'\\n')\n",
        "        i += 1\n",
        "newfile.close()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHnZ2fBBaJzd"
      },
      "source": [
        "def categorical_evaluate(preds, text, predict_table):\n",
        "\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    for predict, word in zip(max_preds, text):\n",
        "        if word == 0:\n",
        "            continue\n",
        "        else:\n",
        "            predict_table.append((word, predict[0]))\n",
        "\n",
        "    return predict_table"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEIg3wLFaK-q"
      },
      "source": [
        "def model_evaluate(model, iterator, predict_table):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_tot = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for text in iterator:\n",
        "            text = text.to(device)\n",
        "            predictions = model(text)\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "\n",
        "            predict_table = categorical_evaluate(predictions, text.view(-1), predict_table)\n",
        "\n",
        "    return predict_table"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq_zSmBxRMUY"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9n2HJWwnIYT"
      },
      "source": [
        "# Predict test set and Save the result as a '.out' file\n",
        "test_X = []\n",
        "sentence = []\n",
        "cnt=0\n",
        "with open('./data/test', \"r\") as test:\n",
        "    for line in test:\n",
        "        if not line.split(): # Ignore a blank line\n",
        "            test_X.append(sentence)\n",
        "            sentence = []\n",
        "            continue\n",
        "        word_type = line.split(\" \")[1]\n",
        "        if word_type in word_to_index:\n",
        "            sentence.append(word_to_index.get(word_type))\n",
        "        else:\n",
        "            unk = unk_preprocessing(word_type) # if the word is not in vocab dictionary, change the word to unknown token\n",
        "            sentence.append(word_to_index.get(unk))\n",
        "    test_X.append(sentence)\n",
        "\n",
        "test_X = pad_features_for_word(test_X, 130) # Padding\n",
        "X_test = torch.LongTensor(test_X)\n",
        "loader_test = DataLoader(X_test, batch_size=10, shuffle=False)\n",
        "\n",
        "evaluate_predict_table2 = []\n",
        "model = BLSTM(INPUT_DIM, \n",
        "              EMBEDDING_DIM, \n",
        "              HIDDEN_DIM, \n",
        "              FIRST_OUTPUT_DIM,\n",
        "              OUTPUT_DIM, \n",
        "              N_LAYERS, \n",
        "              BIDIRECTIONAL, \n",
        "              DROPOUT)\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load('./result/blstm1.pt')) # load pretrained model\n",
        "prediction_table = model_evaluate(model, loader_test, evaluate_predict_table2)\n",
        "\n",
        "term = [int(x[0]) for x in evaluate_predict_table2]\n",
        "y_pred = [int(x[1]) for x in evaluate_predict_table2]\n",
        "\n",
        "# Make test2.out file\n",
        "i=0\n",
        "newfile = open('./result/test1.out', \"w\")\n",
        "with open('./data/test', \"r\") as test:\n",
        "    for line in test:\n",
        "        if not line.split(): # Ignore a blank line\n",
        "            newfile.write('\\n')\n",
        "            continue\n",
        "        index, word_type = line.split(\" \")[0], line.split(\" \")[1].strip('\\n')\n",
        "        for_tag = index_to_ner[y_pred[i]]\n",
        "        newfile.write(str(index)+' '+str(word_type)+' '+for_tag+'\\n')\n",
        "        i += 1\n",
        "newfile.close()\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6EECj6YierW"
      },
      "source": [
        "import pickle\n",
        "# save data\n",
        "with open('./data/vocab_dictionary.pickle','wb') as fw1:\n",
        "    pickle.dump(word_to_index, fw1)\n",
        "with open('./data/ner_dictionary.pickle','wb') as fw2:\n",
        "    pickle.dump(ner_to_index, fw2)\n",
        "with open('./data/int_vocab_dictionary.pickle','wb') as fw3:\n",
        "    pickle.dump(index_to_word, fw3)\n",
        "with open('./data/int_ner_dictionary.pickle','wb') as fw4:\n",
        "    pickle.dump(index_to_ner, fw4)\n",
        "with open('./data/loader_train.pickle','wb') as fw5:\n",
        "    pickle.dump(loader_train, fw5)\n",
        "with open('./data/loader_dev.pickle','wb') as fw6:\n",
        "    pickle.dump(loader_dev, fw6)\n",
        "with open('./data/loader_test.pickle','wb') as fw7:\n",
        "    pickle.dump(loader_test, fw7)    "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfDuKwx2_OZq"
      },
      "source": [
        "checkpoint = {'INPUT_DIM':len(word_to_index),\n",
        "              'EMBEDDING_DIM':100,\n",
        "              'HIDDEN_DIM':256,\n",
        "              'FIRST_OUTPUT_DIM':128,\n",
        "              'OUTPUT_DIM':len(ner_to_index),\n",
        "              'N_LAYERS':1,\n",
        "              'BIDIRECTIONAL':True,\n",
        "              'DROPOUT':0.33,\n",
        "              'state_dict': model.state_dict()}\n",
        "\n",
        "torch.save(checkpoint, 'result/checkpoint.pth')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYibrdwS_2ZZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}